{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DS102 | Self-Study Week 4B - Text Mining IV - Numerically Representing Texts\n",
    "\n",
    "## Learning Objectives\n",
    "At the end of the self-study, you will be able to:\n",
    "\n",
    "- extend the existing implementation of Jaccard Similarity to find similar texts\n",
    "\n",
    "- find additional corpora in the `nltk` library\n",
    "\n",
    "- further understanding of `CountVectorizer`\n",
    "\n",
    "### Datasets Required for this Self-Study\n",
    "1. `billboard-lyrics.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:57.984956Z",
     "start_time": "2018-10-17T17:13:56.979433Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:57.992604Z",
     "start_time": "2018-10-17T17:13:57.987708Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first read from `'billboard-lyrics.csv'` to a `df`. Then, inspect the `df` for all billboard songs in $2015$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.030269Z",
     "start_time": "2018-10-17T17:13:57.996362Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset 1, Credits at the end of the notebook\n",
    "songs_df = pd.read_csv('billboard-lyrics.csv')\n",
    "# Store the indices of the df as a new column 'ID'\n",
    "songs_df['ID'] = songs_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.037849Z",
     "start_time": "2018-10-17T17:13:58.033985Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use tail() to find the last 20 of this dataset. They are the songs in 2015\n",
    "# Write your code here. Solution in Fill in the Blanks 1\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, store all `english` stopwords into `ENGLISH_STOP_WORDS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.048530Z",
     "start_time": "2018-10-17T17:13:58.042201Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use stopwords.words('english') to get all English stopwords and store them in \n",
    "# ENGLISH_STOP_WORDS\n",
    "# This list can also be found in Dataset 1. Credits at the end of the notebook\n",
    "\n",
    "ENGLISH_STOP_WORDS = [] # Solution in Fill in the Blanks 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T15:46:41.623722Z",
     "start_time": "2018-10-17T15:46:41.584166Z"
    },
    "scrolled": true
   },
   "source": [
    "Specify the `ID` of a song to get its song lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.062590Z",
     "start_time": "2018-10-17T17:13:58.052130Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "song_id = 970 # Change the index accordingly.\n",
    "\n",
    "# Use iloc[song_id][<column_name>] to get the values in the song_df. 'ID' is already\n",
    "# done for you.\n",
    "print(songs_df.iloc[song_id]['ID'])\n",
    "# Extract the column 'Lyrics' from the row.  Solution in Fill in the Blanks 3\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:13:47.906906Z",
     "start_time": "2018-10-17T16:13:47.901771Z"
    }
   },
   "source": [
    "Before performing analysis, we can store the values in a dictionary. Before starting, remove all stop words to in the lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.527404Z",
     "start_time": "2018-10-17T17:13:58.067201Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_SONGS = songs_df.shape[0]\n",
    "lyrics = {}\n",
    "\n",
    "for i in range(0, NUM_SONGS):\n",
    "    # Find the song_id from the df row\n",
    "    song_id = songs_df.iloc[i]['ID']\n",
    "    # Your turn: find the Lyrics from the df row\n",
    "    song_lyrics = songs_df.iloc[i]['Lyrics']    \n",
    "    # Use split() to split the words into a list of tokens    \n",
    "    list_of_lyrics = song_lyrics.split()\n",
    "    # Filter for words that exists in the list of stopwords    \n",
    "    list_of_lyrics_without_sw = [w for w in list_of_lyrics if w not in ENGLISH_STOP_WORDS]\n",
    "    # Use the song_id as the key and the lyrics as the value in the dictionary    \n",
    "    lyrics[song_id] = list_of_lyrics_without_sw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.536688Z",
     "start_time": "2018-10-17T17:13:58.530831Z"
    }
   },
   "outputs": [],
   "source": [
    "# Validate the dictionary\n",
    "print(lyrics[song_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a `song_id` and the `lyrics` dataset, return the `song_id` and `score` of the song with the highest similarity score. Note that you do not compare the song to test and itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.553820Z",
     "start_time": "2018-10-17T17:13:58.540886Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_jaccard_score(d1, d2):\n",
    "    set_a, set_b = set(d1), set(d2)\n",
    "    return len(set_a & set_b) / len(set_a | set_b)\n",
    "\n",
    "def get_most_similar_song(lyrics, test_song_id):\n",
    "    highest_song_id = 0\n",
    "    highest_score = 0.0\n",
    "    \n",
    "    # Iterate through every song.\n",
    "    for song_id, song_lyrics_list in lyrics.items():\n",
    "        if song_id != test_song_id: # Do not compare the selected song with itself.\n",
    "            # Reassign highest_song_id and highest_score if song's similarity\n",
    "            # is higher than the current song with highest score. \n",
    "            score = calculate_jaccard_score(lyrics[test_song_id], lyrics[song_id])\n",
    "            if score > highest_score:\n",
    "                highest_song_id, highest_score = song_id, score\n",
    "    \n",
    "    return test_song_id, highest_song_id, highest_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the results to find interesting song pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.636377Z",
     "start_time": "2018-10-17T17:13:58.556955Z"
    }
   },
   "outputs": [],
   "source": [
    "i, j, h_score = get_most_similar_song(lyrics, song_id)\n",
    "# i, j, score = get_most_similar_song(lyrics, songs_df['ID'].sample().iloc[0])\n",
    "#Interesting results: 960, 962, 877, 613, 332, 966. Store them in the song_id variable\n",
    "\n",
    "#Remove the index to show the full list of lyrics\n",
    "print(str(i) + \" (Source) - \" + str(lyrics[i])) \n",
    "print()\n",
    "#Remove the index to show the full list of lyrics\n",
    "print(str(j) + \" (Target) - \" + str(lyrics[j])) \n",
    "print()\n",
    "print(h_score)\n",
    "songs_df[songs_df['ID'].isin([i,j])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Study Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.645364Z",
     "start_time": "2018-10-17T17:13:58.639987Z"
    }
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# Solution: Fill in the Blanks 1\n",
    "#################################\n",
    "# songs_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.653498Z",
     "start_time": "2018-10-17T17:13:58.649392Z"
    }
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# Solution: Fill in the Blanks 2\n",
    "#################################\n",
    "# ENGLISH_STOP_WORDS = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.660498Z",
     "start_time": "2018-10-17T17:13:58.656590Z"
    }
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# Solution: Fill in the Blanks 2\n",
    "#################################\n",
    "# print(songs_df.iloc[song_id]['Lyrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert a Document to a Vector using `CountVectorizer`\n",
    "\n",
    "The following are the words in the corpus in the Na√Øve Bayes Classification example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.669626Z",
     "start_time": "2018-10-17T17:13:58.664317Z"
    }
   },
   "outputs": [],
   "source": [
    "docs_as_s = ['enjoy like', \n",
    "             'enjoy funny happy', \n",
    "             'hate boring like', \n",
    "             'like happy', \n",
    "             'boring dull']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit_transform()` will first fit the dataset to a vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.682471Z",
     "start_time": "2018-10-17T17:13:58.673970Z"
    }
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "ft = count_vectorizer.fit_transform(docs_as_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see the vocabulary. Every unique term in the corpus is assigned a position in the dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.693358Z",
     "start_time": "2018-10-17T17:13:58.685700Z"
    }
   },
   "outputs": [],
   "source": [
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positions are useful when finding if the word exists in the particular document. For example, if the column with index `2` has a value greater than `0` then the document will contain the word `enjoy`. \n",
    "\n",
    "**NOTE: ** The position of the words are random and hence the description does not fit the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.707065Z",
     "start_time": "2018-10-17T17:13:58.697298Z"
    }
   },
   "outputs": [],
   "source": [
    "ft.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Exploration - `nltk.corpus`\n",
    "\n",
    "`nltk.corpus` has many corpora (plural form of corpus) to allow you to download text to play with. The following are 2 more contemporary corpora. Before you access the corpus, ensure that you use `nltk.download()` to download the corpus to your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">The Brown corpus is curated by Brown University. It has 1 million words in English and contains text from 500 sources. Each source is categorised into a genre.</alert>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:58.731105Z",
     "start_time": "2018-10-17T17:13:58.711513Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "try:\n",
    "    #Use corpus.categories() to show all category tags of each document.\n",
    "    print(brown.categories())\n",
    "except LookupError:\n",
    "    print(\"Downloading brown...\")    \n",
    "    nltk.download('brown')\n",
    "    print(\"Download brown complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">The Reuters Corpus contains 10,788 news documents totaling 1.3 million words. The documents have been classified into 90 topics.</alert>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T17:13:59.417108Z",
     "start_time": "2018-10-17T17:13:58.738972Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "try:\n",
    "    #Use corpus.words() to show all words that appear in the corpus.\n",
    "    print(reuters.words()[:20])\n",
    "except LookupError:\n",
    "    print(\"Downloading reuters...\")      \n",
    "    nltk.download('reuters')\n",
    "    print(\"Download reuters complete\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on the corpora mentioned, head over to [Chapter 2 of Natural Language Processing with Python](http://www.nltk.org/book/ch02.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credits**\n",
    "- [sebleier](https://gist.github.com/sebleier/554280) for Dataset 1\n",
    "- [Kaggle (Billboard 1964-2015 Songs + Lyrics)](https://www.kaggle.com/rakannimer/billboard-lyrics) for Dataset 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
