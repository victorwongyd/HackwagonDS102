{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DS102 | In Class Practice Week 4B - Text Mining II - Gaining Insights from Text\n",
    "<hr>\n",
    "## Learning Objectives\n",
    "At the end of the lesson, you will be able to:\n",
    "\n",
    "- use **Jaccard Similarity** to find similar texts\n",
    "\n",
    "- perform **sentiment analysis** using the `SentimentIntensityAnalyzer`\n",
    "\n",
    "- train a **Na誰ve Bayes classifier** to classify a new piece of text into 2 classes\n",
    "\n",
    "### Datasets Required for this Self-Study\n",
    "1. `billboard-lyrics.csv`\n",
    "\n",
    "2. `popcorn-reviews-5k.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:28:18.553745Z",
     "start_time": "2018-10-17T16:28:17.221324Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "#If you are running this for the first time, use the next cell to download all the \n",
    "# corpora first\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:28:18.568228Z",
     "start_time": "2018-10-17T16:28:18.557130Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/victorwongyd/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the VADER list of words / lexicon\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Similarity\n",
    "\n",
    "Jaccard Similarity is used to show how similar two documents are. Given two documents, $A$ and $B$, the Jaccard Similarity Score is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Jaccard Similarity Score} = \\frac{A\\cap B}{A \\cup B}\n",
    "$$\n",
    "\n",
    "Simply put, the numerator is the number of words that **are common across both documents** and the denominator is the **total number of words in both documents**. Keep in mind that the words here refer to **unique words**.\n",
    "\n",
    "The function below, `calculate_jaccard_score` will return the similarity score of two documents, `d1` and `d2`. It uses list comprehensions and the documenation for that can be found [here](https://docs.python.org/3/tutorial/datastructures.html). Also, find out more about how multiple variables can be declared in the same line [here](https://docs.python.org/3.6/tutorial/datastructures.html#tuples-and-sequences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:28:18.596673Z",
     "start_time": "2018-10-17T16:28:18.590076Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_jaccard_score(d1, d2):\n",
    "    set_a, set_b = set(d1), set(d2)\n",
    "    return len(set_a & set_b) / len(set_a | set_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:28:18.613668Z",
     "start_time": "2018-10-17T16:28:18.600324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'would', 'like', 'to', 'consolidate', 'a', 'few', 'of', 'my', 'higher', 'interest', 'rate', 'credit', 'cards.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19047619047619047"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = 'I would like to consolidate a few of my higher interest rate credit cards.'.split()\n",
    "s2 = 'this loan is to consolidate credit card debt and pay of debt'.split()\n",
    "# Your turn: what is the Jaccard score of the 2 lists? Call the function for this\n",
    "# to validate your answer.\n",
    "print(s1)\n",
    "calculate_jaccard_score(s1, s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis with `nltk`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nltk` library has a sentiment analyser. It uses the VADER method or **Valence Aware Dictionary for\n",
    "sEntiment Reasoning**. It is a lexicon (vocabulary) of words and their relative sentiment strength. For example:\n",
    "    \n",
    "- `Good` has a positive but weak score, while `Excellent` scores more\n",
    "- `Bad` has a negative but weaks score, while `Tragedy` scores more\n",
    "\n",
    "Use `sid.polarity_scores(t)` to find the sentiment of a text. Then, use the `compound` value to determine the overall score. Note that `compound` give a (normalised) value from $-1$ to $1$, and hence a positive number is good sentiment while a negative number is bad sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:28:18.655462Z",
     "start_time": "2018-10-17T16:28:18.618981Z"
    }
   },
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how the sentiment scores change based on the sentiment of a movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:28:18.668274Z",
     "start_time": "2018-10-17T16:28:18.658844Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset 3, Credits at the end of the notebook\n",
    "# This is an example of a positive review, showing positive sentiment.\n",
    "review_1 = \"\"\"I thoroughly enjoyed this movie because there was a genuine sincerity in the acting.\"\"\"\n",
    "ss = sid.polarity_scores(review_1)\n",
    "print(ss)\n",
    "print(ss['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:28:18.682911Z",
     "start_time": "2018-10-17T16:28:18.672068Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset 3, Credits at the end of the notebook\n",
    "review_2 = \"I found it really boring and silly.\"\n",
    "# Exercise: What is the compound score of the above review?\n",
    "#\n",
    "\n",
    "# This is an example of a negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:28:18.694934Z",
     "start_time": "2018-10-17T16:28:18.685971Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dataset 3, Credits at the end of the notebook\n",
    "review_3 = \"My personal favorite horror film.\"\n",
    "# Your turn: What is the compound score of the above review?\n",
    "#\n",
    "\n",
    "# Your turn: Is this a positve or negative movie review? \n",
    "# What does the VADER polarity score say about this review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Na誰ve Bayes Classification\n",
    "\n",
    "We now extend sentiment analysis to texts that we have never seen before, and use a machine learning algorithm - Na誰ve Bayes Classification - to help us predict if a newly received review has positive or negative sentiment. Na誰ve Bayes Classification is the first machine learning algorithm we will learn in DS102.\n",
    "\n",
    "#### PROBLEM SETUP\n",
    "\n",
    "Consider you have the following documents and their tagged sentiment **class**. $1$ represents a positive sentiment while $0$ represents a negative sentiment. There are only 2 possible classes in this problem.\n",
    "\n",
    "|ID | Text | Sentiment\n",
    "|---|---|--\n",
    "|1|`enjoy like`|$1$\n",
    "|2|`enjoy funny happy`|$1$\n",
    "|3|`hate boring like`|$0$\n",
    "|4|`like happy`|$1$\n",
    "|5|`boring dull`|$0$\n",
    "\n",
    "We now have a new document which is `like happy`. What is the predicted class of this new document?\n",
    "\n",
    "#### TRAINING\n",
    " \n",
    "The model mostly revolves around counting words and multiplying these proportions / probabilities. The following calculations are performed:\n",
    "\n",
    "1. Find the number of unique words and store them in a variable $V$, called the vocabulary. $|V|$ is the length of the vocabulary.\n",
    "2. Calculate the probability of each class, $1$ and $0$.\n",
    "3. Calculate the **conditional probability** of each word given a class. For this calculation, add $1$ to the numerator and add $|V|$ to the denominator.\n",
    "\n",
    "In this case, \n",
    "- $V$ = `{'boring', 'dull', 'enjoy', 'funny', 'happy', 'hate', 'like'}` and hence $|V|= 7$\n",
    "\n",
    "- $P(1) = \\frac{3}{5}$ and  $P(0) = \\frac{2}{5}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19679300291545188\n",
      "0.0462962962962963\n"
     ]
    }
   ],
   "source": [
    "# like, happy, joy\n",
    "# Positive class\n",
    "print(3/5 *3/14 *3/14 *1/14*100)\n",
    "print(2/5 *2/12 *1/12 *1/12*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The conditional probability $P($ `enjoy` $| 1)$ is the number of times `enjoy` appears in class $1$ divided by the total number of words in class $1$. The number of times `enjoy` appears is $2$. The total number of words is $7$. Remember to \"smooth\" the fraction. Hence, $P($ `enjoy` $| 1) = \\frac{2+1}{7+7} = \\frac{3}{14}$. Repeat this for the rest of the words in both classes:\n",
    "\n",
    "|Word | Class $1$ calculation or $P($ `(word)` $| 1)$| Class $0$ calculation or $P($ `(word` $| 0)$\n",
    "|---|---|--\n",
    "|`boring`|$\\frac{1}{14}$|$\\frac{3}{12}$\n",
    "|`dull`|$\\frac{1}{14}$ |$\\frac{2}{12}$\n",
    "|`enjoy`|$\\frac{3}{14}$ |$\\frac{1}{12}$\n",
    "|`funny`|$\\frac{2}{14}$|$\\frac{1}{12}$\n",
    "|`happy`|$\\frac{3}{14}$|$\\frac{1}{12}$\n",
    "|`hate`|$\\frac{1}{14}$|$\\frac{2}{12}$\n",
    "|`like`|$\\frac{3}{14}$|$\\frac{2}{12}$\n",
    "\n",
    "#### TEST\n",
    "\n",
    "For a the document `like happy`, calculate the probability scores of each class. Do so by multiplying the probability of the class and the conditional probability of each term in each class. For $1$, the calculation is:\n",
    "\n",
    "$$\n",
    "\\text{Class 1 Prediction} \\propto P(1) \\times P(\\text{ like } | 1) \\times P(\\text{ happy } | 1) = \\frac{3}{5} \\times \\frac{3}{14} \\times \\frac{3}{14} = 0.02755\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:28:18.723348Z",
     "start_time": "2018-10-17T16:28:18.698063Z"
    }
   },
   "outputs": [],
   "source": [
    "(3/5) * (3/14) * (3/14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for $0$, the calculation is:\n",
    "$$\n",
    "\\text{Class 0 Prediction} \\propto P(0) \\times P(\\text{ like } | 0) \\times P(\\text{ happy } | 0) = \\frac{2}{5} \\times \\frac{1}{12} \\times \\frac{2}{12} = 0.0055\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:28:18.741028Z",
     "start_time": "2018-10-17T16:28:18.733201Z"
    }
   },
   "outputs": [],
   "source": [
    "(2/5) * (1/12) * (2/12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the score for class $1$ is higher, using the model, the document is classified as class $1$ or positive sentiment. \n",
    "\n",
    "#### YOUR TURN\n",
    "There is a new document `hate dull`. What is the predicted class of this document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:28:18.763323Z",
     "start_time": "2018-10-17T16:28:18.750569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003061224489795918"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: What is the calculation for Class 1?\n",
    "3/5 * 1/14 * 1/14 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:28:18.779437Z",
     "start_time": "2018-10-17T16:28:18.770573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011111111111111112"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise: What is the calculation for Class 0?\n",
    "2/5 * 2/12 * 2/12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applied Na誰ve Bayes Classification using `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:28:18.849643Z",
     "start_time": "2018-10-17T16:28:18.784153Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5196_9</td>\n",
       "      <td>Human Tornado (1976) is in many ways a better ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2668_9</td>\n",
       "      <td>Chilling, majestic piece of cinematic fright, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9565_3</td>\n",
       "      <td>I cant say that Wargames The Dead Code is the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                             review  sentiment\n",
       "0  5196_9  Human Tornado (1976) is in many ways a better ...          1\n",
       "1  2668_9  Chilling, majestic piece of cinematic fright, ...          1\n",
       "2  9565_3  I cant say that Wargames The Dead Code is the ...          0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4500</th>\n",
       "      <td>2910_7</td>\n",
       "      <td>This is a great film for McCartneys and Beatle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4501</th>\n",
       "      <td>11707_10</td>\n",
       "      <td>I remember seeing this movie a long time ago, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4502</th>\n",
       "      <td>5461_7</td>\n",
       "      <td>Escaping the life of being pimped by her fathe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                             review  sentiment\n",
       "4500    2910_7  This is a great film for McCartneys and Beatle...          1\n",
       "4501  11707_10  I remember seeing this movie a long time ago, ...          1\n",
       "4502    5461_7  Escaping the life of being pimped by her fathe...          1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset 3, Credits at the end of the notebook\n",
    "# Read the reviews data from the CSV\n",
    "popcorn_df = pd.read_csv('popcorn-reviews-5k.csv', sep=\"#\") \n",
    "\n",
    "# Split the dataset into the training and test set. \n",
    "# The first 4500 records will be the training set while the last 500 records will be \n",
    "# the test set.\n",
    "d_train = popcorn_df[:4500]\n",
    "display(d_train.head(3))\n",
    "d_test = popcorn_df[4500:]\n",
    "display(d_test.head(3))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:28:18.869475Z",
     "start_time": "2018-10-17T16:28:18.852082Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your turn: print the first 5 records of the training set. \n",
    "# How many columns are there in the training set?\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:28:18.891106Z",
     "start_time": "2018-10-17T16:28:18.874714Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your turn: print the first 5 records of the test set. \n",
    "# How many columns are there in the training set?\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN\n",
    "\n",
    "Train the model given the reviews and the given sentiment. Recall that for `sentiment`, 1 represents a positive review and 0 represents a negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:28:19.660655Z",
     "start_time": "2018-10-17T16:28:18.900083Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using fit_transform, transform the corpus to a matrix.\n",
    "count_vect = CountVectorizer()\n",
    "train_df_counts = count_vect.fit_transform(train_df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:28:19.683337Z",
     "start_time": "2018-10-17T16:28:19.663918Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train a multinomial classifier using the training set using the features \n",
    "# and the training set labels\n",
    "clf = MultinomialNB().fit(train_df_counts, train_df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST\n",
    "\n",
    "Now that we have trained our classifier, let's test it using the test set. We will check the actual prediction of a test example, and observe what the predicted model gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = d_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:28:19.698481Z",
     "start_time": "2018-10-17T16:28:19.687226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeremy Brett is simply the best Holmes ever, narrowly edging out the great Basil Rathbone of course, and this is probably the best adaptation of a Conon-Doyle short story. Excellent performances all round, especially from Robert Hardy, and both Brett and Hardwick fully rounded and comfortable in their roles makes this a superb piece of drama.\n",
      "1\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-550bc24db1f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Let's now see what class the model predicts for this test example.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "# Now, randomly sample an example from the test set.\n",
    "t_sample = test_df.sample()\n",
    "\n",
    "# Let's see the review in the test set and the actual sentiment.\n",
    "s = t_sample.iloc[0]['review']\n",
    "print(s)\n",
    "t = t_sample.iloc[0]['sentiment']\n",
    "print(t)\n",
    "\n",
    "# Let's now see what class the model predicts for this test example.\n",
    "print()\n",
    "print(clf.predict(count_vect.transform([s])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credits**\n",
    "- [sebleier](https://gist.github.com/sebleier/554280) for Dataset 1\n",
    "- [Kaggle (Billboard 1964-2015 Songs + Lyrics)](https://www.kaggle.com/rakannimer/billboard-lyrics) for Dataset 2\n",
    "- [Kaggle (Bag of Words Meets Bags of Popcorn)](https://www.kaggle.com/c/word2vec-nlp-tutorial/data) for Dataset 3\n",
    "\n",
    "**Footnote**\n",
    "\n",
    "(1) : The reviews are partially processed. Only removal of special characters was performed. The remaining steps to be performed are text normalisation.\n",
    "\n",
    "**Further Reading**\n",
    "Na誰ve Bayes can also be used for the following classification problems:\n",
    "\n",
    "1. Spam vs. Non-Spam in E-Mail Filtering\n",
    "2. Product classification using product titles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
