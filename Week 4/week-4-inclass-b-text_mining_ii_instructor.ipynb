{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DS102 | In Class Practice Week 4B - Text Mining II - Gaining Insights from Text\n",
    "<hr>\n",
    "## Learning Objectives\n",
    "At the end of the lesson, you will be able to:\n",
    "\n",
    "- use **Jaccard Similarity** to find similar texts\n",
    "\n",
    "- perform **sentiment analysis** using the `SentimentIntensityAnalyzer`\n",
    "\n",
    "- train a **Naïve Bayes classifier** to classify a new piece of text into 2 classes\n",
    "\n",
    "### Datasets Required for this Self-Study\n",
    "1. `billboard-lyrics.csv`\n",
    "\n",
    "2. `popcorn-reviews-5k.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:37.253101Z",
     "start_time": "2018-09-12T15:21:35.526668Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "#If you are running this for the first time, use the next cell to download all the corpora first\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:37.259953Z",
     "start_time": "2018-09-12T15:21:37.255779Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download the VADER list of words / lexicon\n",
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**define `ENGLISH_STOP_WORDS` as a list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:37.280285Z",
     "start_time": "2018-09-12T15:21:37.263840Z"
    }
   },
   "outputs": [],
   "source": [
    "#Dataset 1, Credits at the end of the notebook\n",
    "ENGLISH_STOP_WORDS = ['i', 'me', 'my', 'myself', 'we', 'our', \n",
    "                      'ours', 'ourselves', 'you', 'your', 'yours', \n",
    "                      'yourself', 'yourselves', 'he', 'him', 'his', \n",
    "                      'himself', 'she', 'her', 'hers', 'herself', 'it', \n",
    "                      'its', 'itself', 'they', 'them', 'their', 'theirs', \n",
    "                      'themselves', 'what', 'which', 'who', 'whom', 'this', \n",
    "                      'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "                      'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', \n",
    "                      'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', \n",
    "                      'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n",
    "                      'with', 'about', 'against', 'between', 'into', 'through', 'during', \n",
    "                      'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', \n",
    "                      'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', \n",
    "                      'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "                      'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', \n",
    "                      'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Similarity\n",
    "\n",
    "Jaccard Similarity is used to show how similar two documents are. Given two documents, $A$ and $B$, the Jaccard Similarity Score is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Jaccard Similarity Score} = \\frac{A\\cap B}{A \\cup B}\n",
    "$$\n",
    "\n",
    "Simply put, the numerator is the number of words that **are common across both documents** and the denominator is the **total number of words in both documents**. Keep in mind that the words here refer to **unique words**.\n",
    "\n",
    "The function below, `calculate_jaccard_score` will return the similarity score of two documents, `d1` and `d2`. It uses list comprehensions and the documenation for that can be found [here](https://docs.python.org/3/tutorial/datastructures.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:37.289923Z",
     "start_time": "2018-09-12T15:21:37.283505Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_jaccard_score(d1, d2):\n",
    "    intersect = set([l for l in d1 if l in d2])\n",
    "    union = set(d1 + d2)\n",
    "    return len(intersect)/len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:37.301068Z",
     "start_time": "2018-09-12T15:21:37.294964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n"
     ]
    }
   ],
   "source": [
    "s1 = ['one', 'dem', 'days', 'dont', 'take', 'personal',]\n",
    "s2 = ['dont', 'take', 'personal', 'wanna', 'alone',]\n",
    "# Your turn: what is the Jaccard score of the 2 lists? Call the function for this\n",
    "print(calculate_jaccard_score(s1, s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple application of Jaccard Similarity is to find similar lyrics in songs. The following is a collection of song lyrics of some songs from 1965 to 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:37.341895Z",
     "start_time": "2018-09-12T15:21:37.305679Z"
    }
   },
   "outputs": [],
   "source": [
    "#Dataset 2, Credits at the end of the notebook\n",
    "songs_df = pd.read_csv('billboard-lyrics.csv', index_col=0)\n",
    "# Set the ID of the df to be the index\n",
    "songs_df['ID'] = songs_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:37.381797Z",
     "start_time": "2018-09-12T15:21:37.344274Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Song</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Year</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>11</td>\n",
       "      <td>cheerleader</td>\n",
       "      <td>omi</td>\n",
       "      <td>2015</td>\n",
       "      <td>when i need motivation my one solution is my ...</td>\n",
       "      <td>966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>12</td>\n",
       "      <td>cant feel my face</td>\n",
       "      <td>the weeknd</td>\n",
       "      <td>2015</td>\n",
       "      <td>and i know shell be the death of me at least ...</td>\n",
       "      <td>967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>13</td>\n",
       "      <td>love me like you do</td>\n",
       "      <td>ellie goulding</td>\n",
       "      <td>2015</td>\n",
       "      <td>youre the light youre the night youre the col...</td>\n",
       "      <td>968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>14</td>\n",
       "      <td>take me to church</td>\n",
       "      <td>hozier</td>\n",
       "      <td>2015</td>\n",
       "      <td>my lovers got humour shes the giggle at a fun...</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>15</td>\n",
       "      <td>bad blood</td>\n",
       "      <td>taylor swift featuring kendrick lamar</td>\n",
       "      <td>2015</td>\n",
       "      <td>cause baby now we got bad blood you know it u...</td>\n",
       "      <td>970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>16</td>\n",
       "      <td>lean on</td>\n",
       "      <td>major lazer and dj snake featuring mo</td>\n",
       "      <td>2015</td>\n",
       "      <td>do you recall not long ago we would walk on t...</td>\n",
       "      <td>971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>17</td>\n",
       "      <td>want to want me</td>\n",
       "      <td>jason derulo</td>\n",
       "      <td>2015</td>\n",
       "      <td>its too hard to sleep i got the sheets on the...</td>\n",
       "      <td>972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>18</td>\n",
       "      <td>shake it off</td>\n",
       "      <td>taylor swift</td>\n",
       "      <td>2015</td>\n",
       "      <td>i stay up too late got nothing in my brain th...</td>\n",
       "      <td>973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>19</td>\n",
       "      <td>where are \"u now</td>\n",
       "      <td>skrillex and diplo featuring justin bieber</td>\n",
       "      <td>2015</td>\n",
       "      <td>i need you i need you i need you i need you i...</td>\n",
       "      <td>974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>20</td>\n",
       "      <td>fight song</td>\n",
       "      <td>rachel platten</td>\n",
       "      <td>2015</td>\n",
       "      <td>like a small boat on the ocean sending big wa...</td>\n",
       "      <td>975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Rank                 Song                                      Artist  \\\n",
       "966    11          cheerleader                                         omi   \n",
       "967    12    cant feel my face                                  the weeknd   \n",
       "968    13  love me like you do                              ellie goulding   \n",
       "969    14    take me to church                                      hozier   \n",
       "970    15            bad blood       taylor swift featuring kendrick lamar   \n",
       "971    16              lean on       major lazer and dj snake featuring mo   \n",
       "972    17      want to want me                                jason derulo   \n",
       "973    18         shake it off                                taylor swift   \n",
       "974    19     where are \"u now  skrillex and diplo featuring justin bieber   \n",
       "975    20           fight song                              rachel platten   \n",
       "\n",
       "     Year                                             Lyrics   ID  \n",
       "966  2015   when i need motivation my one solution is my ...  966  \n",
       "967  2015   and i know shell be the death of me at least ...  967  \n",
       "968  2015   youre the light youre the night youre the col...  968  \n",
       "969  2015   my lovers got humour shes the giggle at a fun...  969  \n",
       "970  2015   cause baby now we got bad blood you know it u...  970  \n",
       "971  2015   do you recall not long ago we would walk on t...  971  \n",
       "972  2015   its too hard to sleep i got the sheets on the...  972  \n",
       "973  2015   i stay up too late got nothing in my brain th...  973  \n",
       "974  2015   i need you i need you i need you i need you i...  974  \n",
       "975  2015   like a small boat on the ocean sending big wa...  975  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your turn: Inspect the last 10 rows of the dataset to see songs in 2015 using tail()\n",
    "songs_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `df.shape` to get the number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:37.393055Z",
     "start_time": "2018-09-12T15:21:37.385217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(976, 6)\n",
      "976\n"
     ]
    }
   ],
   "source": [
    "# Your turn: Use .shape to find the number of rows and columns\n",
    "print(songs_df.shape)\n",
    "# Your turn: Assign the number of rows to the variable num_songs\n",
    "num_songs = songs_df.shape[0]\n",
    "print(num_songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract out the values in the `df` by using `iloc` first to obtain the row by its position in the `df`. Use the second index to extract the value by column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:37.409028Z",
     "start_time": "2018-09-12T15:21:37.397296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "\n",
      " when youre alone and life is making you lonely you can always go downtown when youve got worries all the noise and the hurry seems to help i know downtownjust listen to the music of the traffic in the city linger on the sidewalk where the neon signs are pretty how can you lose the lights are much brighter there you can forget all your troubles forget all your caresso go downtown things will be great when youre downtown no finer place for sure downtown every things waiting for youdont hang around and let your problems surround you there are movie shows downtown maybe you know some little places to go to where they never close downtownjust listen to the rhythm of a gentle bossa nova youll be dancing with em too before the night is over happy again the lights are much brighter there you can forget all your troubles forget all your caresso go downtown where all the lights are bright downtown waiting for you tonight downtown youre gonna be alright nowdowntownand you may find somebody kind to help and understand you someone who is just like you and needs a gentle hand to guide them along so maybe ill see you there we can forget all our troubles forget all our caresso go downtown things will be great when youre downtown dont wait a minute more downtown everything is waiting for you downtown downtown downtown downtown \n"
     ]
    }
   ],
   "source": [
    "print(songs_df.iloc[4]['ID'])\n",
    "print()\n",
    "# Your turn: Extract the column 'Lyrics' from the row\n",
    "print(songs_df.iloc[4]['Lyrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing analysis, we can store the values in a dictionary. Before starting, remove all stop words to in the lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:38.341490Z",
     "start_time": "2018-09-12T15:21:37.412382Z"
    }
   },
   "outputs": [],
   "source": [
    "lyrics = {}\n",
    "\n",
    "for i in range(0, num_songs):\n",
    "    # Find the song_id from the df row\n",
    "    song_id = songs_df.iloc[i]['ID']\n",
    "    # Your turn: find the Lyrics from the df row\n",
    "    song_lyrics = songs_df.iloc[i]['Lyrics']    \n",
    "    # Use split() to split the words into a list of tokens    \n",
    "    list_of_lyrics = song_lyrics.split()\n",
    "    # Filter for words that exists in the list of stopwords    \n",
    "    list_of_lyrics_without_sw = [w for w in list_of_lyrics if w not in ENGLISH_STOP_WORDS]\n",
    "    # Your turn: Use the song_id as the key and the lyrics as the value in the dictionary    \n",
    "    lyrics[song_id] = list_of_lyrics_without_sw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this to validate the dictionary `lyrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:38.349792Z",
     "start_time": "2018-09-12T15:21:38.343766Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['youre', 'alone', 'life', 'making', 'lonely', 'always', 'go', 'downtown', 'youve', 'got', 'worries', 'noise', 'hurry', 'seems', 'help', 'know', 'downtownjust', 'listen', 'music', 'traffic', 'city', 'linger', 'sidewalk', 'neon', 'signs', 'pretty', 'lose', 'lights', 'much', 'brighter', 'forget', 'troubles', 'forget', 'caresso', 'go', 'downtown', 'things', 'great', 'youre', 'downtown', 'finer', 'place', 'sure', 'downtown', 'every', 'things', 'waiting', 'youdont', 'hang', 'around', 'let', 'problems', 'surround', 'movie', 'shows', 'downtown', 'maybe', 'know', 'little', 'places', 'go', 'never', 'close', 'downtownjust', 'listen', 'rhythm', 'gentle', 'bossa', 'nova', 'youll', 'dancing', 'em', 'night', 'happy', 'lights', 'much', 'brighter', 'forget', 'troubles', 'forget', 'caresso', 'go', 'downtown', 'lights', 'bright', 'downtown', 'waiting', 'tonight', 'downtown', 'youre', 'gonna', 'alright', 'nowdowntownand', 'may', 'find', 'somebody', 'kind', 'help', 'understand', 'someone', 'like', 'needs', 'gentle', 'hand', 'guide', 'along', 'maybe', 'ill', 'see', 'forget', 'troubles', 'forget', 'caresso', 'go', 'downtown', 'things', 'great', 'youre', 'downtown', 'dont', 'wait', 'minute', 'downtown', 'everything', 'waiting', 'downtown', 'downtown', 'downtown', 'downtown']\n"
     ]
    }
   ],
   "source": [
    "# Validate the dictionary\n",
    "print(lyrics[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** - Given a `song_id` and the `lyrics` dataset, return the `song_id` and `score` of the song with the highest similarity score. Note that you do not compare the song to test with itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:38.362014Z",
     "start_time": "2018-09-12T15:21:38.354560Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your turn: Complete this method to return the highest song ID and the highest Jaccard score\n",
    "def get_most_similar_song(lyrics, test_song_id):\n",
    "    highest_song_id = 0\n",
    "    highest_score = 0.0\n",
    "    print(highest_score)\n",
    "#     for song_id, song_lyrics_list in lyrics.items():\n",
    "#         if song_id != test_song_id:\n",
    "#             score = calculate_jaccard_score(lyrics[test_song_id], lyrics[song_id])\n",
    "#             if score > highest_score:\n",
    "#                 highest_song_id, highest_score = song_id, score\n",
    "    \n",
    "    return test_song_id, highest_song_id, highest_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the results to find interesting song pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:38.392829Z",
     "start_time": "2018-09-12T15:21:38.366574Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "960 (Source) - ['im', 'hurting', 'baby', 'im', 'broken', 'need', 'loving', 'loving', 'need', 'im', 'without', 'im', 'something', 'weak', 'got', 'begging', 'begging', 'im', 'kneesi', 'dont', 'wanna', 'needing', 'love', 'wanna', 'deep', 'love', 'killing', 'youre', 'away', 'ooh', 'baby', 'cause', 'really', 'dont', 'care', 'wanna', 'gotta', 'get', 'one', 'little', 'tasteyour', 'sugar', 'yes', 'please', 'wont', 'come', 'put', 'im', 'right', 'cause', 'need', 'little', 'love', 'little', 'sympathy', 'yeah', 'show', 'good', 'loving', 'make', 'alright', 'need', 'little', 'sweetness', 'life', 'sugar', 'yes', 'please', 'wont', 'come', 'put', 'memy', 'broken', 'pieces', 'pick', 'dont', 'leave', 'hanging', 'hanging', 'come', 'give', 'im', 'without', 'ya', 'im', 'insecure', 'one', 'thing', 'one', 'thing', 'im', 'living', 'fori', 'dont', 'wanna', 'needing', 'love', 'wanna', 'deep', 'love', 'killing', 'youre', 'away', 'ooh', 'baby', 'cause', 'really', 'dont', 'care', 'wanna', 'gotta', 'get', 'one', 'little', 'tasteyour', 'sugar', 'yes', 'please', 'wont', 'come', 'put', 'im', 'right', 'cause', 'need', 'little', 'love', 'little', 'sympathy', 'yeah', 'show', 'good', 'loving', 'make', 'alright', 'need', 'little', 'sweetness', 'life', 'sugar', 'sugar', 'yes', 'please', 'yes', 'please', 'wont', 'come', 'put', 'meyeah', 'want', 'red', 'velvet', 'want', 'sugar', 'sweet', 'dont', 'let', 'nobody', 'touch', 'unless', 'somebodys', 'gotta', 'man', 'aint', 'way', 'cause', 'girl', 'youre', 'hotter', 'southern', 'california', 'bayi', 'dont', 'wanna', 'play', 'games', 'dont', 'gotta', 'afraid', 'dont', 'give', 'shy', 'shit', 'make', 'thats', 'mysugar', 'yes', 'please', 'wont', 'come', 'put', 'oh', 'right', 'right', 'cause', 'need', 'need', 'little', 'love', 'little', 'sympathy', 'yeah', 'show', 'good', 'loving', 'make', 'alright', 'need', 'little', 'sweetness', 'life', 'sugar', 'sugar', 'yes', 'please', 'yes', 'please', 'wont', 'come', 'put', 'meyour', 'sugar', 'yes', 'please', 'wont', 'come', 'put', 'im', 'right', 'cause', 'need', 'little', 'love', 'little', 'sympathy', 'yeah', 'show', 'good', 'loving', 'make', 'alright', 'need', 'little', 'sweetness', 'life', 'sugar', 'yes', 'please', 'wont', 'come', 'put']\n",
      "\n",
      "0 (Target) - ['sam', 'sham', 'miscellaneous', 'wooly', 'bully', 'wooly', 'bully', 'sam', 'sham', 'pharaohs', 'domingo', 'samudio', 'uno', 'dos', 'one', 'two', 'tres', 'quatro', 'matty', 'told', 'hatty', 'thing', 'saw', 'two', 'big', 'horns', 'wooly', 'jaw', 'wooly', 'bully', 'wooly', 'bully', 'wooly', 'bully', 'wooly', 'bully', 'wooly', 'bully', 'hatty', 'told', 'matty', 'lets', 'dont', 'take', 'chance', 'lets', 'belseven', 'come', 'learn', 'dance', 'wooly', 'bully', 'wooly', 'bully', 'wooly', 'bully', 'wooly', 'bully', 'wooly', 'bully', 'matty', 'told', 'hatty', 'thats', 'thing', 'get', 'someone', 'really', 'pull', 'wool', 'wooly', 'bully', 'wooly', 'bully', 'wooly', 'bully', 'wooly', 'bully', 'wooly', 'bully', 'lseven', 'letter', 'l', 'number', '7', 'typed', 'form', 'rough', 'square', 'l7', 'lyrics', 'mean', 'lets', 'square']\n",
      "\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Song</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Year</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>wooly bully</td>\n",
       "      <td>sam the sham and the pharaohs</td>\n",
       "      <td>1965</td>\n",
       "      <td>sam the sham miscellaneous wooly bully wooly b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>5</td>\n",
       "      <td>sugar</td>\n",
       "      <td>maroon 5</td>\n",
       "      <td>2015</td>\n",
       "      <td>im hurting baby im broken down i need your lo...</td>\n",
       "      <td>960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Rank         Song                         Artist  Year  \\\n",
       "0       1  wooly bully  sam the sham and the pharaohs  1965   \n",
       "960     5        sugar                       maroon 5  2015   \n",
       "\n",
       "                                                Lyrics   ID  \n",
       "0    sam the sham miscellaneous wooly bully wooly b...    0  \n",
       "960   im hurting baby im broken down i need your lo...  960  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i, j, h_score = get_most_similar_song(lyrics, 960)\n",
    "# i, j, score = get_most_similar_song(lyrics, songs_df['ID'].sample().iloc[0])\n",
    "#Interesting results: 960, 962, 877, 613, 332, 966\n",
    "\n",
    "#Remove the index to show the full list of lyrics\n",
    "print(str(i) + \" (Source) - \" + str(lyrics[i])) \n",
    "print()\n",
    "#Remove the index to show the full list of lyrics\n",
    "print(str(j) + \" (Target) - \" + str(lyrics[j])) \n",
    "print()\n",
    "print(h_score)\n",
    "songs_df[songs_df['ID'].isin([i,j])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis with `nltk`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nltk` library has a sentiment analyser. It uses the VADER method or **Valence Aware Dictionary for\n",
    "sEntiment Reasoning**. It is a lexicon (vocabulary) of words and their relative sentiment strength. For example:\n",
    "    \n",
    "- `Good` has a positive but weak score, while `Excellent` scores more\n",
    "- `Bad` has a negative but weaks score, while `Tragedy` scores more\n",
    "\n",
    "Use `sid.polarity_scores(t)` to find the sentiment of a text. Then, use the `compound` value to determine the overall score. Note that `compound` give a (normalised) value from $-1$ to $1$, and hence a positive number is good sentiment while a negative number is bad sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:38.422284Z",
     "start_time": "2018-09-12T15:21:38.396955Z"
    }
   },
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how the sentiment scores change based on the sentiment of a movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:38.435448Z",
     "start_time": "2018-09-12T15:21:38.428728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.754, 'pos': 0.246, 'compound': 0.5563}\n",
      "0.5563\n"
     ]
    }
   ],
   "source": [
    "#Dataset 3, Credits at the end of the notebook\n",
    "# This is an example of a positive review, showing positive sentiment.\n",
    "review_1 = \"\"\"I thoroughly enjoyed this movie because there was a genuine sincerity in the acting.\"\"\"\n",
    "ss = sid.polarity_scores(review_1)\n",
    "print(ss)\n",
    "print(ss['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:38.447983Z",
     "start_time": "2018-09-12T15:21:38.439076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.326, 'neu': 0.503, 'pos': 0.171, 'compound': -0.3025}\n",
      "-0.3025\n"
     ]
    }
   ],
   "source": [
    "# Dataset 3, Credits at the end of the notebook\n",
    "review_2 = \"I found it really boring and silly.\"\n",
    "# Your turn: Print the VADER polarity scores. What is the compound score of the above review?\n",
    "ss2 = sid.polarity_scores(review_2)\n",
    "print(ss2)\n",
    "print(ss2['compound'])\n",
    "# This is an example of a negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:38.460755Z",
     "start_time": "2018-09-12T15:21:38.452801Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.381, 'neu': 0.309, 'pos': 0.309, 'compound': -0.1779}\n",
      "-0.1779\n"
     ]
    }
   ],
   "source": [
    "#Dataset 3, Credits at the end of the notebook\n",
    "review_3 = \"My personal favorite horror film.\"\n",
    "# Your turn: Print the VADER polarity scores. What is the compound score of the above review?\n",
    "ss3 = sid.polarity_scores(review_3)\n",
    "print(ss3)\n",
    "print(ss3['compound'])\n",
    "# Your turn: Is this a positve or negative movie review? What does the VADER polarity score\n",
    "# say about this review?\n",
    "# A review could have a positive sentiment but because of the words\n",
    "# in it, it could result in a negative sentiment score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes Classification\n",
    "\n",
    "We now extend sentiment analysis to texts that we have never seen before, and use a machine learning algorithm - **Naïve Bayes Classification** - to help us predict if a newly received review has positive or negative sentiment. Naïve Bayes Classification is the first machine learning algorithm we will learn in DS102.\n",
    "\n",
    "#### PROBLEM SETUP\n",
    "\n",
    "Consider you have the following documents and their tagged sentiment **class**. $1$ represents a positive sentiment while $0$ represents a negative sentiment. There are only 2 possible classes in this problem. These documents are possible because stop words have been already removed.\n",
    "\n",
    "|ID | Text | Sentiment\n",
    "|---|---|--\n",
    "|1|`enjoy like`|$1$\n",
    "|2|`enjoy funny happy`|$1$\n",
    "|3|`hate boring like`|$0$\n",
    "|4|`like happy`|$1$\n",
    "|5|`boring dull`|$0$\n",
    "\n",
    "We now have a new document which is `like happy`. What is the predicted class of this new document?\n",
    "\n",
    "#### TRAINING\n",
    " \n",
    "The model mostly revolves around counting words and multiplying these proportions / probabilities. The following calculations are performed:\n",
    "\n",
    "1. Find the number of unique words and store them in a variable $V$, called the vocabulary. $|V|$ is the length of the vocabulary.\n",
    "2. Calculate the probability of each class, $1$ and $0$.\n",
    "3. Calculate the **conditional probability** of each word given a class. For this calculation, add $1$ to the numerator and add $|V|$ to the denominator.\n",
    "\n",
    "In this case, \n",
    "- $V$ = `{'boring', 'dull', 'enjoy', 'funny', 'happy', 'hate', 'like'}` and hence $|V|= 7$\n",
    "\n",
    "- $P(1) = \\frac{3}{5}$ and  $P(0) = \\frac{2}{5}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The conditional probability $P($ `enjoy` $| 1)$ is the number of times `enjoy` appears in class $1$ divided by the total number of words in class $1$. The number of times `enjoy` appears is $2$. The total number of words is $7$. Remember to \"smooth\" the fraction. Hence, $P($ `enjoy` $| 1) = \\frac{2+1}{7+7} = \\frac{3}{14}$. Repeat this for the rest of the words in both classes:\n",
    "\n",
    "|Word | Class $1$ calculation or $P($ `(word)` $| 1)$| Class $0$ calculation or $P($ `(word` $| 0)$\n",
    "|---|---|--\n",
    "|`boring`|$\\frac{1}{14}$|$\\frac{3}{12}$\n",
    "|`dull`|$\\frac{1}{14}$ |$\\frac{2}{12}$\n",
    "|`enjoy`|$\\frac{3}{14}$ |$\\frac{1}{12}$\n",
    "|`funny`|$\\frac{2}{14}$|$\\frac{1}{12}$\n",
    "|`happy`|$\\frac{3}{14}$|$\\frac{1}{12}$\n",
    "|`hate`|$\\frac{1}{14}$|$\\frac{2}{12}$\n",
    "|`like`|$\\frac{3}{14}$|$\\frac{2}{12}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:00:42.437395Z",
     "start_time": "2018-09-12T15:00:42.428971Z"
    }
   },
   "source": [
    "#### TEST\n",
    "\n",
    "For a the document `like happy`, calculate the probability scores of each class. Do so by multiplying the probability of the class and the conditional probability of each term in each class. For $1$, the calculation is:\n",
    "\n",
    "$$\n",
    "\\text{Class 1 Prediction} \\propto P(1) \\times P(\\text{ like } | 1) \\times P(\\text{ happy } | 1) = \\frac{3}{5} \\times \\frac{3}{14} \\times \\frac{3}{14} = 0.02755\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:38.474361Z",
     "start_time": "2018-09-12T15:21:38.465839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027551020408163263"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3/5) * (3/14) * (3/14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for $0$, the calculation is:\n",
    "$$\n",
    "\\text{Class 0 Prediction} \\propto P(0) \\times P(\\text{ like } | 0) \\times P(\\text{ happy } | 0) = \\frac{2}{5} \\times \\frac{1}{12} \\times \\frac{2}{12} = 0.0055\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:38.485841Z",
     "start_time": "2018-09-12T15:21:38.478089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005555555555555555"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2/5) * (1/12) * (2/12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the score for class $1$ is higher, using the model, the document is classified as class $1$ or positive sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BAYES THEOREM EXPLAINED\n",
    "\n",
    "$$P(A,B) = P(A)\\times P(B|A)$$\n",
    "\n",
    "$$\\begin{align}\n",
    "P (A|B) &= \\frac{P(A,B)}{P(B)}\\\\\n",
    "&= \\frac{P(A)\\times P(B|A)}{P(B)}\n",
    "\\end{align}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GENERAL FORM OF THE ALGORITHM\n",
    "\n",
    "Given a document with $n$ terms, $w_1, w_2, \\cdots, w_n$, calculate the prediction of a class, $C_k$, using the formula:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{Class } C_k \\text{ Prediction } = P(C_k| w_1,w_2,\\cdots,w_n)&= \\frac{P(\\text{Class } C_k) \\times P(w_1|C_k) \\times P(w_2|C_k) \\times \\cdots \\times P(w_n|C_k)}{P(w_1) \\times P(w_2) \\times \\cdots \\times P(w_n)}\n",
    "\\end{align}$$\n",
    "\n",
    "and find the class $C_k$ with the **largest probability**. In this case, $k=0$ or $k=1$.\n",
    "\n",
    "Notice that for all classes $C_0, C_1$, the denominator term \n",
    "$$P(w_1) \\times P(w_2) \\times \\cdots \\times P(w_n)$$ \n",
    "\n",
    "is the same. Hence, it can be treated as a constant, let's call it $T$. The above equation simplifies to:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{Class } C_k \\text{ Prediction } = \\frac 1T \\times \\begin{bmatrix}P(\\text{Class } C_k) \\times P(w_1|C_k) \\times P(w_2|C_k) \\times \\cdots \\times P(w_n|C_k)\\end{bmatrix}\n",
    "\\end{align}$$\n",
    "\n",
    "and hence can be treated simply as a proportionality constant, yielding the equation we used above:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "$$\\begin{align}\n",
    "\\text{Class } C_k \\text{ Prediction } \\propto P(\\text{Class } C_k) \\times P(w_1|C_k) \\times P(w_2|C_k) \\times \\cdots \\times P(w_n|C_k)\n",
    "\\end{align}$$</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR TURN\n",
    "There is a new document `hate dull`. What is the predicted class of this document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:38.498333Z",
     "start_time": "2018-09-12T15:21:38.490138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0030612244897959178"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your turn: What is the calculation for Class 1?\n",
    "(3/5) * (1/14) * (1/14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:38.509338Z",
     "start_time": "2018-09-12T15:21:38.502056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01111111111111111"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your turn: What is the calculation for Class 0?\n",
    "(2/5) * (2/12) * (2/12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applied Naïve Bayes Classification using `sklearn`\n",
    "\n",
    "What is the predicted sentiment of the following partially processed (Footnote 1) reviews? \n",
    "\n",
    "- `Ive seen it over and over throughout the years and Im always spellbound by it`\n",
    "\n",
    "- `This film failed to explore the humanity of the animals which left me with an empty feeling inside.`\n",
    "\n",
    "- `As an adult I really did enjoy this one. `\n",
    "\n",
    "- `The lead characters  lone wolf  bravado is uninspiring and lame, and the script was apparently written by a monkey with an eight grade education.`\n",
    "\n",
    "The reviews are so because you have identified words (or a group of words) that reflect this sentiment. `spellbound`, `failed`, `enjoy`, `uninspiring`, `lame` are words that give you an indication of the sentiment of the text. Let's use this idea in application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:38.580857Z",
     "start_time": "2018-09-12T15:21:38.513213Z"
    }
   },
   "outputs": [],
   "source": [
    "#Dataset 3, Credits at the end of the notebook\n",
    "# Read the reviews data from the CSV\n",
    "popcorn_df = pd.read_csv('popcorn-reviews-5k.csv', sep=\"#\") \n",
    "\n",
    "# Split the dataset into the training and test set. The first 4500 records will be the training set\n",
    "# while the last 500 records will be the test set.\n",
    "train_df = popcorn_df[:4500]\n",
    "test_df = popcorn_df[4500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:38.595336Z",
     "start_time": "2018-09-12T15:21:38.583663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                             review  sentiment\n",
      "0    5196_9  Human Tornado (1976) is in many ways a better ...          1\n",
      "1    2668_9  Chilling, majestic piece of cinematic fright, ...          1\n",
      "2    9565_3  I cant say that Wargames The Dead Code is the ...          0\n",
      "3  10271_10  This movie should not be compared to  The Stin...          1\n",
      "4    5639_7  Ive read the other reviews and found some to b...          1\n",
      "(4500, 3)\n"
     ]
    }
   ],
   "source": [
    "# Your turn: print the first 5 records of the training set. How many columns are there in the training set?\n",
    "print(train_df.head())\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:38.609900Z",
     "start_time": "2018-09-12T15:21:38.598138Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id                                             review  sentiment\n",
      "4500    2910_7  This is a great film for McCartneys and Beatle...          1\n",
      "4501  11707_10  I remember seeing this movie a long time ago, ...          1\n",
      "4502    5461_7  Escaping the life of being pimped by her fathe...          1\n",
      "4503    6029_7  There arent too many times when I see a film a...          1\n",
      "4504    9462_1  Inappropriate. The PG rating that this movie g...          0\n",
      "(500, 3)\n"
     ]
    }
   ],
   "source": [
    "# Your turn: print the first 5 records of the test set. How many columns are there in the training set?\n",
    "print(test_df.head())\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN\n",
    "\n",
    "Train the model given the reviews and the given sentiment. Recall that for `sentiment`, 1 represents a positive review and 0 represents a negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:39.267302Z",
     "start_time": "2018-09-12T15:21:38.614464Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using fit_transform, transform the corpus to a matrix.\n",
    "count_vect = CountVectorizer()\n",
    "train_df_counts = count_vect.fit_transform(train_df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:39.284326Z",
     "start_time": "2018-09-12T15:21:39.269960Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train a multinomial classifier using the training set using the features and the training set labels\n",
    "clf = MultinomialNB().fit(train_df_counts, train_df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST\n",
    "\n",
    "Now that we have trained our classifier, let's test it using the test set. We will check the actual prediction of a test example, and observe what the predicted model gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T15:21:39.299032Z",
     "start_time": "2018-09-12T15:21:39.286906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For my humanities quarter project for school, i chose to do human trafficking. After some research on the internet, i found this DVD and ordered it. I just finished watching it and I am still thinking about it. All I can say is  Wow . It is such a compelling story of a 12 year old Vietnamese girl named Holly and an American man named Patric who tries to save her. The ending leaves you breathless, and although its not a happily-ever-after ending, it is very realistic. It is amazing and I recommend it to anyone! You really connect with Holly and Patric and your heart breaks for her and because of what happens to her. I loved it so much and now I want to know what happens next \n",
      "1\n",
      "\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# Now, randomly sample an example from the test set.\n",
    "t_sample = test_df.sample()\n",
    "\n",
    "# Let's see the review in the test set and the actual sentiment.\n",
    "s = t_sample.iloc[0]['review']\n",
    "print(s)\n",
    "t = t_sample.iloc[0]['sentiment']\n",
    "print(t)\n",
    "\n",
    "# Let's now see what class the model predicts for this test example.\n",
    "print()\n",
    "print(clf.predict(count_vect.transform([s])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credits**\n",
    "- [sebleier](https://gist.github.com/sebleier/554280) for Dataset 1\n",
    "- [Kaggle (Billboard 1964-2015 Songs + Lyrics)](https://www.kaggle.com/rakannimer/billboard-lyrics) for Dataset 2\n",
    "- [Kaggle (Bag of Words Meets Bags of Popcorn)](https://www.kaggle.com/c/word2vec-nlp-tutorial/data) for Dataset 3\n",
    "\n",
    "**Footnote**\n",
    "\n",
    "(1) : The reviews are partially processed. Only removal of special characters was performed. The remaining steps to be performed are stemming and removal of stop words.\n",
    "\n",
    "**Further Reading**\n",
    "Naïve Bayes can also be used for the following classification problems:\n",
    "\n",
    "1. Spam vs. Non-Spam in E-Mail Filtering\n",
    "2. Product classification using product titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
