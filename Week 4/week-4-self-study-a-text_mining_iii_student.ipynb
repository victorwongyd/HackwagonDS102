{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DS102 | Self-Study Week 4A - Text Mining III - POS Tagging and Lemmatisation\n",
    "<hr>\n",
    "\n",
    "## Learning Objectives\n",
    "At the end of the self-study, you will be able to:\n",
    "\n",
    "- list tags from a Part-of-Speech tagger related to verbs, nouns, prepositions, adverbs, adjectives and d\n",
    "\n",
    "- interpret the output of a **Part-of-Speech tagger**\n",
    "\n",
    "- define a **lemma** as a word that can be found in a dictionary\n",
    "\n",
    "- identify lemmatisation a way to find the root form of a word\n",
    "\n",
    "At the end of the lesson, you will be able to:\n",
    "\n",
    "- use `pos_tag` from `nltk` to perform Part-of-Speech (POS) tagging of a sentence\n",
    "\n",
    "- given a word and its POS tag, use `WordNetLemmatizer`'s `lemmatize` from `nltk.stem` to find its corresponding lemma \n",
    "\n",
    "### Datasets Required for this Self-Study\n",
    "1. `loans-descs-1k.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T15:46:41.519786Z",
     "start_time": "2018-10-17T15:46:40.489724Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T15:46:41.530462Z",
     "start_time": "2018-10-17T15:46:41.523037Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T15:46:41.557427Z",
     "start_time": "2018-10-17T15:46:41.536254Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset 1: Credits at the end of the notebook\n",
    "df = pd.read_csv('loans-descs-1k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, let's first revise the steps to stem a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T15:46:41.577462Z",
     "start_time": "2018-10-17T15:46:41.563129Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the raw text into 2 sentences that can be used for processing\n",
    "s1 = df['desc'][4]\n",
    "print(s1)\n",
    "s1 = re.sub('Borrower added on \\d+/\\d+/\\d+ >|<br>', '', s1)\n",
    "s1 = s1.strip()\n",
    "print()\n",
    "print(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T15:46:41.623722Z",
     "start_time": "2018-10-17T15:46:41.584166Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts1 = word_tokenize(s1)\n",
    "print(ts1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stemming result will be useful for later comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T15:46:41.647389Z",
     "start_time": "2018-10-17T15:46:41.634752Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_words = []\n",
    "for t in ts1:\n",
    "    u = stemmer.stem(t)\n",
    "    stemmed_words.append(u)\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider another algorithm to find the root form of a word, called **Lemmatisation**. Before we start talking about Lemmatisation, we need to first understand **Part-of-Speech (POS) Tagging**. \n",
    "\n",
    "### Part-of-Speech (POS) Tagging\n",
    "\n",
    "POS tagging is a way to group a word into its **class**. Commonly, a word and tag pair is represented as a tuple. We will use one of `nltk`'s tagged corpora, in particular the *Penn Treebank Project* to help us tag newly discovered words. To find the POS tag of a word, use `nltk.pos_tag(word_tokens)`.\n",
    "\n",
    "Notice that the resulting value consists of many tuples. The first element in the tuple is the original word from the sentence and the second element is the assigned POS tag. Refer to this [link](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) to understand the meaning of each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T15:46:41.865423Z",
     "start_time": "2018-10-17T15:46:41.654792Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tagged_words_by_treebank = nltk.pos_tag(ts1)\n",
    "print(tagged_words_by_treebank)\n",
    "# Tags to be aware of: PRP, VBP, VBG, NN, VBN, NNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first letter of the tag represent similar classes. In particular, \n",
    "\n",
    "- the pattern `N[A-Z]+` represents nouns and \n",
    "\n",
    "- the pattern `V[A-Z]+` represent verbs\n",
    "\n",
    "Hence, we can take the first character and convert it to lower case. This first letter of the tag will be used for Lemmatisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T15:49:01.854704Z",
     "start_time": "2018-10-17T15:49:01.849710Z"
    }
   },
   "outputs": [],
   "source": [
    "tagged_words = []\n",
    "for twt in tagged_words_by_treebank:\n",
    "    # Get the first element of the tuple, and the first letter of the second element\n",
    "    # of the tuple.\n",
    "    tagged_words.append((twt[0], twt[1][0].lower()))\n",
    "print(tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation\n",
    "\n",
    "A **Lemma** is a word found in the dictionary. Hence, you can think of them as the root form of a word. Given a word and its corresponding tag, we can find the word's root form in English. This will be easier for human interpretation. Use `WordNetLemmatizer.lemmatize(term, pos=tag)` to find the root form of the word given the source word and its associated POS tag.\n",
    "\n",
    "Note that if the POS tag cannot be found, a `KeyError` will be thrown. For example, the first word will have the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T15:51:49.603028Z",
     "start_time": "2018-10-17T15:51:49.599705Z"
    }
   },
   "outputs": [],
   "source": [
    "# lemmatiser = WordNetLemmatizer()\n",
    "# lemmatiser.lemmatize('I', pos='p') # Uncomment this line to see the KeyError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, wrap the Lemmatisation step into `try...catch` block so the program continues even if a `KeyError` is encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T15:52:50.621875Z",
     "start_time": "2018-10-17T15:52:50.614863Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "lemmed_words = []\n",
    "for tw_pair in tagged_words:\n",
    "    tw_word, tw_tag = tw_pair[0], tw_pair[1]\n",
    "    lemm_word = tw_word\n",
    "    try:\n",
    "        lemm_word = lemmatiser.lemmatize(tw_word, pos=tw_tag)\n",
    "    except KeyError:\n",
    "        # If an error is thrown, then the word is assumed to be in its root form.\n",
    "        print(\"KeyError: \" + tw_word)\n",
    "        pass\n",
    "\n",
    "    lemmed_words.append(lemm_word)\n",
    "print(lemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the original form of the sentence, the result after stemming and the result after lemmatisation for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T15:46:44.279163Z",
     "start_time": "2018-10-17T15:46:44.270061Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"%15s   %15s   %15s\" % (\"Raw\", \"Stemming\", \"Lemmatisation\"))\n",
    "print(\"%15s-- %15s-- %15s\" % (\"------------\", \"------------\", \"--------------\"))\n",
    "for i in range(0, len(stemmed_words)-1):\n",
    "    print (\"%15s   %15s   %15s\" % (ts1[i], stemmed_words[i], lemmed_words[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Exploration - Exercise on POS Tagging & Lemmatisation\n",
    "\n",
    "Pick one of the following 2 sentences. This is also from the `loans-descs-1k.csv` dataset. Perform POS tagging, followed by Lemmatisation of your selected sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T15:46:44.298497Z",
     "start_time": "2018-10-17T15:46:44.293089Z"
    }
   },
   "outputs": [],
   "source": [
    "s2 = \"I really need to consolidate my credit card debt so that I can become debt free. The interest is killing me and I'm just not getting anywhere with the balances. Help!\"\n",
    "s3 = \"Hello, I just closed on the house of my dreams and I would like to use this loan to pay off my high interest credit cards and build a deck on my home.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: First, perform tokenization using word_tokenize\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Then, perform POS tagging\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Then, perform Lemmatisation\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T16:03:23.207104Z",
     "start_time": "2018-10-17T16:03:23.193924Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example Solution:\n",
    "s_x = '''This loan request is to consolidate loans to a lower intrest rate with a goal of becoming debt free. I have been employed with the same company for 10 years next month. I pay ahead on my loans. My goal is to be debt free in 10 years, including my home.'''\n",
    "\n",
    "# Tokenization & POS Tagggin\n",
    "tks = nltk.pos_tag(nltk.word_tokenize(s_x))\n",
    "print(tks)\n",
    "print()\n",
    "\n",
    "# Lemmatisation\n",
    "final_w = []\n",
    "for tw_pair in tks:\n",
    "    tw_word, tw_tag = tw_pair[0], tw_pair[1][0].lower()\n",
    "    lemm_word = tw_word\n",
    "    try:\n",
    "        lemm_word = lemmatiser.lemmatize(tw_word, pos=tw_tag)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    final_w.append(lemm_word)\n",
    "\n",
    "# Show the result after Lemmatisation\n",
    "print(\"%15s %15s\" % (\"ORIGINAL\", \"LEMMATISATION\"))\n",
    "for i in range(0, len(final_w)):\n",
    "    print(\"%15s %15s\" % (tks[i][0], final_w[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on the corpora mentioned, head over to [Chapter 2 of Natural Language Processing with Python](http://www.nltk.org/book/ch02.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credits**\n",
    "- [Kaggle](https://www.kaggle.com/wendykan/lending-club-loan-data) for Dataset 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
