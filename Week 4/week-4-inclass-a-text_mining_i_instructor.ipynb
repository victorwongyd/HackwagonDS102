{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DS102 | In Class Practice Week 4A - Text Mining I - Text Normalisation\n",
    "<hr>\n",
    "\n",
    "## Pre-Requisites\n",
    "\n",
    "1. Self-Study Week 3A - Cleaning Strings\n",
    "\n",
    "## Learning Objectives\n",
    "At the end of the lesson, you will be able to:\n",
    "\n",
    "- define **corpus**, **documents** and **terms** as part of the study of Natural Language Processing\n",
    "\n",
    "- define **tokenisation** as breaking a document into terms\n",
    "\n",
    "- understand the definition of **root form** of a word for verbs and nouns\n",
    "\n",
    "- identify **stemming** as a way to find the root form of a word\n",
    "\n",
    "- learn how to use **stop words** to filter out terms in a document that is not meaningful\n",
    "\n",
    "At the end of the lesson, you will be able to:\n",
    "\n",
    "- perform simple tokenisation of a document into terms using `split()`\n",
    "\n",
    "- use `word_tokenize` from `nltk.tokenize` to break a document into a list of words\n",
    "\n",
    "- use `PorterStemmer`'s `stem` from `nltk.stem` to perform stemming of words\n",
    "\n",
    "- retrieve a list of stopwords defined in `stopwords.words()` from `nltk.corpus`\n",
    "\n",
    "### Datasets Required for this Self-Study\n",
    "1. `songs-100.csv`\n",
    "\n",
    "2. `loans-descs-1k.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.055872Z",
     "start_time": "2018-09-12T14:45:11.641104Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.062218Z",
     "start_time": "2018-09-12T14:45:13.058429Z"
    }
   },
   "outputs": [],
   "source": [
    "#If you are running this for the first time, use the next cell to download all the corpora first\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.073903Z",
     "start_time": "2018-09-12T14:45:13.065417Z"
    }
   },
   "outputs": [],
   "source": [
    "#Use this cell to download all the required corpora first\n",
    "# try:\n",
    "#     from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "#     from nltk import pos_tag, wordnet\n",
    "#     from nltk.tokenize import word_tokenize\n",
    "#     from nltk.corpus import stopwords\n",
    "# except LookupError:\n",
    "#     print(\"Downloading corpora...\")    \n",
    "#     nltk.download('punkt')\n",
    "#     nltk.download('stopwords')\n",
    "#     nltk.download('averaged_perceptron_tagger')\n",
    "#     nltk.download('wordnet')\n",
    "#     nltk.download('stopwords')\n",
    "#     print(\"Downloads complete. Rerun this line.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus, Documents and Terms\n",
    "\n",
    "In lingustics (the study of language), a **corpus** is a collection of texts, represented by documents. A **document** contains multiple words and when strung together, produce meaning. Each word is called a **term**. \n",
    "\n",
    "Consider the following corpus of 4 documents from the 100 Song Titles dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.086566Z",
     "start_time": "2018-09-12T14:45:13.080957Z"
    }
   },
   "outputs": [],
   "source": [
    "#Dataset 1, Credits at the end of the notebook\n",
    "song_titles = ['shape of you', 'paris', 'scared to be lonely', 'symphony feat zara larsson',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each <u>song title is a document</u>. The <u>collection of song titles forms the corpus</u>. The first document `shape of you` has <u>3 terms</u>. The second document `paris` has <u>1 term</u>. The third document `scared to be lonely` has <u>4 terms</u>.\n",
    "<hr>\n",
    "We now read from the dataset CSV file. As you can see, there are 100 documents in the song titles corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.103592Z",
     "start_time": "2018-09-12T14:45:13.091250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                          Shape of You\n",
      "1                     Despacito - Remix\n",
      "2    Despacito (Featuring Daddy Yankee)\n",
      "3              Something Just Like This\n",
      "4                           I'm the One\n",
      "5                               HUMBLE.\n",
      "6       It Ain't Me (with Selena Gomez)\n",
      "Name: name, dtype: object\n",
      "--- ---\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Dataset 1: Credits at the end of the notebook\n",
    "song_titles_df = pd.read_csv('songs-100.csv', index_col=0)\n",
    "print(song_titles_df['name'].head(7))\n",
    "print(\"--- ---\")\n",
    "print(song_titles_df['name'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.139070Z",
     "start_time": "2018-09-12T14:45:13.106510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Havana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Look What You Made Me Do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Call On Me - Ryan Riback Extended Remix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Pretty Girl - Cheat Codes X CADE Remix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I Don’t Wanna Live Forever (Fifty Shades Darke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Sign of the Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Say You Won't Let Go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1-800-273-8255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Side To Side</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Starboy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Believer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Passionfruit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XO TOUR Llif3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>24K Magic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Chantaje</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Don't Wanna Know (feat. Kendrick Lamar)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Starboy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Ahora Dice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Slow Hands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>DNA.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Friends (with BloodPop®)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Passionfruit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Slide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm the One</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Scared to Be Lonely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>It Ain't Me (with Selena Gomez)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Castle on the Hill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Ahora Dice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>How Far I'll Go - From \"Moana\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Felices los 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Say You Won't Let Go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>24K Magic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>goosebumps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XO TOUR Llif3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Your Song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Reggaetón Lento (Bailemos)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1-800-273-8255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Despacito (Featuring Daddy Yankee)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>rockstar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>DNA.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Passionfruit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Too Good At Goodbyes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Strip That Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XO TOUR Llif3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>It Ain't Me (with Selena Gomez)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>New Rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shape of You</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name\n",
       "49                                             Havana\n",
       "78                           Look What You Made Me Do\n",
       "46                                           Location\n",
       "41            Call On Me - Ryan Riback Extended Remix\n",
       "56             Pretty Girl - Cheat Codes X CADE Remix\n",
       "9   I Don’t Wanna Live Forever (Fifty Shades Darke...\n",
       "70                                  Sign of the Times\n",
       "22                               Say You Won't Let Go\n",
       "35                                     1-800-273-8255\n",
       "83                                       Side To Side\n",
       "31                                            Starboy\n",
       "19                                           Believer\n",
       "36                                       Passionfruit\n",
       "10                                      XO TOUR Llif3\n",
       "59                                          24K Magic\n",
       "47                                           Chantaje\n",
       "65            Don't Wanna Know (feat. Kendrick Lamar)\n",
       "31                                            Starboy\n",
       "92                                         Ahora Dice\n",
       "67                                         Slow Hands\n",
       "60                                               DNA.\n",
       "93                           Friends (with BloodPop®)\n",
       "36                                       Passionfruit\n",
       "33                                              Slide\n",
       "4                                         I'm the One\n",
       "27                                Scared to Be Lonely\n",
       "6                     It Ain't Me (with Selena Gomez)\n",
       "17                                 Castle on the Hill\n",
       "92                                         Ahora Dice\n",
       "66                     How Far I'll Go - From \"Moana\"\n",
       "44                                      Felices los 4\n",
       "82                                               Weak\n",
       "22                               Say You Won't Let Go\n",
       "59                                          24K Magic\n",
       "71                                         goosebumps\n",
       "46                                           Location\n",
       "10                                      XO TOUR Llif3\n",
       "91                                          Your Song\n",
       "81                         Reggaetón Lento (Bailemos)\n",
       "35                                     1-800-273-8255\n",
       "2                  Despacito (Featuring Daddy Yankee)\n",
       "37                                           rockstar\n",
       "60                                               DNA.\n",
       "36                                       Passionfruit\n",
       "76                               Too Good At Goodbyes\n",
       "38                                    Strip That Down\n",
       "10                                      XO TOUR Llif3\n",
       "6                     It Ain't Me (with Selena Gomez)\n",
       "34                                          New Rules\n",
       "0                                        Shape of You"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your turn: Draw a sample of 50 observations from the dataset, with replacement. Use replace=True for this\n",
    "song_titles_df.sample(50, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Tokenisation using `split()`\n",
    "\n",
    "Before starting, store the song titles as a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.149880Z",
     "start_time": "2018-09-12T14:45:13.143493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shape of You', 'Despacito - Remix', 'Despacito (Featuring Daddy Yankee)', 'Something Just Like This', \"I'm the One\", 'HUMBLE.', \"It Ain't Me (with Selena Gomez)\", 'Unforgettable', \"That's What I Like\", 'I Don’t Wanna Live Forever (Fifty Shades Darker) - From \"Fifty Shades Darker (Original Motion Picture Soundtrack)\"']\n"
     ]
    }
   ],
   "source": [
    "song_titles_list_0 = song_titles_df['name'].tolist()\n",
    "print(song_titles_list_0[:10]) #Remove this index to show all titles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T14:28:00.996711Z",
     "start_time": "2018-07-31T14:28:00.991120Z"
    }
   },
   "source": [
    "Before performing any langugage processing, we will first perform tokenisation. **Tokenisation** is breaking up a long string into individual words or terms. For simple tokenisation, Python's `split()` method is used. \n",
    "\n",
    "In its raw form, the document `Despacito (Featuring Daddy Yankee)` has 4 terms. When tokenisation is performed without any prior processing, the 2nd and 4th term, `(Featuring` and `Yankee)`, **contains** special characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.162866Z",
     "start_time": "2018-09-12T14:45:13.155700Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Despacito', '(Featuring', 'Daddy', 'Yankee)']\n"
     ]
    }
   ],
   "source": [
    "# Your turn: use split() on position 2 from the song_titles_list_0\n",
    "print(song_titles_list_0[2].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1** - We first remove special chracters using the `re.sub()` function and the pattern `[.®'&$’\\\"\\-()]`. This pattern simply means capturing **any** of the special chracters in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.177920Z",
     "start_time": "2018-09-12T14:45:13.166790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shape of You', 'Despacito  Remix', 'Despacito Featuring Daddy Yankee', 'Something Just Like This', 'Im the One', 'HUMBLE', 'It Aint Me with Selena Gomez', 'Unforgettable', 'Thats What I Like', 'I Dont Wanna Live Forever Fifty Shades Darker  From Fifty Shades Darker Original Motion Picture Soundtrack']\n"
     ]
    }
   ],
   "source": [
    "song_titles_list_1 = []\n",
    "# Define the regex\n",
    "special_chars_p = \"[.®'&$’\\\"\\-()]\"\n",
    "for s in song_titles_list_0:\n",
    "    # Your turn: perform the substitution using re.sub()\n",
    "    s1 = re.sub(special_chars_p, '', s)\n",
    "    # Your turn: append this string, after substitution to the list    \n",
    "    song_titles_list_1.append(s1)\n",
    "print(song_titles_list_1[:10]) #Remove the indices to see all the songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2** - The second step is to convert all words to lowercase using `lower()`. Then, use `split()` to perform tokenisation and collect all terms in a document.\n",
    "\n",
    "<div class=\"alert alert-info\">**Additional Notes**: Further steps can be done, for example, substituting letters with accents like `ú` in `rehúso` to letters without, i.e. `u`. However they are not in DS102 syllabus. Refer to [this StackOverflow answer](https://stackoverflow.com/questions/33328645/how-to-remove-accent-in-python-3-5-and-get-a-string-with-unicodedata-or-other-so) to help you.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:48.613993Z",
     "start_time": "2018-09-12T14:45:48.608219Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shape', 'of', 'you']\n",
      "['despacito', 'remix']\n",
      "['despacito', 'featuring', 'daddy', 'yankee']\n",
      "['something', 'just', 'like', 'this']\n",
      "['im', 'the', 'one']\n",
      "['humble']\n",
      "['it', 'aint', 'me', 'with', 'selena', 'gomez']\n",
      "['unforgettable']\n",
      "['thats', 'what', 'i', 'like']\n",
      "['i', 'dont', 'wanna', 'live', 'forever', 'fifty', 'shades', 'darker', 'from', 'fifty', 'shades', 'darker', 'original', 'motion', 'picture', 'soundtrack']\n"
     ]
    }
   ],
   "source": [
    "song_titles_list_2 = []\n",
    "\n",
    "for s1 in song_titles_list_1:\n",
    "    # Your turn: Convert all words to lower case\n",
    "    s2 = s1.lower()\n",
    "    # Your turn: Use split() to convert the string into a list of tokens\n",
    "    s2 = s2.split()\n",
    "    # Then, append this to song_titles_list_2\n",
    "    song_titles_list_2.append(s2)\n",
    "    \n",
    "for s2 in song_titles_list_2[:10]: #Remove the indices to see all the songs.\n",
    "    print(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation using `nltk`'s `word_tokenize()`\n",
    "\n",
    "The Natural Language Toolkit or `nltk` library is a very powerful library used for natural language processing. We will be using the `word_tokenize()` function to perform tokenisation and `pos_tag` to perform Part-of-Speech or POS tagging of words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.219533Z",
     "start_time": "2018-09-12T14:45:13.203555Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset 2: Credits at the end of the notebook\n",
    "# Your turn: read the dataset from loans-descs-1k.csv into df \n",
    "df = pd.read_csv('loans-descs-1k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.230550Z",
     "start_time": "2018-09-12T14:45:13.222762Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Borrower added on 02/14/14 > I am consolidating credit card debt incurred over three years ago and having a concrete end in sight is more motivating.  I am eagerly striving towards becoming completely debt free.<br>\n"
     ]
    }
   ],
   "source": [
    "#Convert the raw text into 2 sentences that can be used for processing\n",
    "s1 = df['desc'][4]\n",
    "print(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.243856Z",
     "start_time": "2018-09-12T14:45:13.234924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Borrower added on 02/14/14 > I am consolidating credit card debt incurred over three years ago and having a concrete end in sight is more motivating.  I am eagerly striving towards becoming completely debt free.<br>\n",
      "\n",
      "I am consolidating credit card debt incurred over three years ago and having a concrete end in sight is more motivating.  I am eagerly striving towards becoming completely debt free.\n"
     ]
    }
   ],
   "source": [
    "print(s1)\n",
    "\n",
    "r = 'Borrower added on \\d+/\\d+/\\d+ >|<br>'\n",
    "# Your turn: use re.sub() to substitute the string with that pattern to an empty string\n",
    "s1 = re.sub('Borrower added on \\d+/\\d+/\\d+ >|<br>', '', s1)\n",
    "# Your turn: use strip() to remove leading and trailing spaces\n",
    "s1 = s1.strip()\n",
    "\n",
    "print()\n",
    "print(s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tokenise a sentence, simply use `word_tokenize()` from the `nltk` library. This will convert the sentence into individual words.\n",
    "\n",
    "<div class=\"alert alert-info\">**Additional Notes**: There are other tokenisers from `nltk` like `RegexpTokenizer`. They will split text specifically when they find a particular regular expression. Particularly useful for money expressions. However they are not in DS102 syllabus. Refer to [the API](https://www.nltk.org/api/nltk.tokenize.html) to find out more.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.273869Z",
     "start_time": "2018-09-12T14:45:13.247208Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'consolidating', 'credit', 'card', 'debt', 'incurred', 'over', 'three', 'years', 'ago', 'and', 'having', 'a', 'concrete', 'end', 'in', 'sight', 'is', 'more', 'motivating', '.', 'I', 'am', 'eagerly', 'striving', 'towards', 'becoming', 'completely', 'debt', 'free', '.']\n"
     ]
    }
   ],
   "source": [
    "# Your turn: Use word_tokenize(string) to convert the string into a list of tokens.\n",
    "# Assign this to a new variable called ts1\n",
    "ts1 = word_tokenize(s1)\n",
    "print(ts1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguistics - The root form of a word (verbs & nouns)\n",
    "Stemming is one way to infer the **root form of a word**. We will only limit our discussion to verbs (action words) and nouns (naming words). First, consider the following 3 sentences that use different forms of the word `watch`:\n",
    "\n",
    "- `Larry watches television.` (singular present tense)\n",
    "\n",
    "- `The children watch television.` (simple tense / plural, present tense)\n",
    "\n",
    "- `My son is watching television.` (present participle tense / present continuous tense)\n",
    "\n",
    "The word `watch` exists in 3 different <u>**forms of the verb**</u> as they exist in different tenses. However, algorithms treat them as **separate words** during analysis. Hence, we need to find the root form of the verb so they can be treated as the same word during analysis as they have the same meaning, in this case `watch`. \n",
    "\n",
    "Now consider the next 2 sentences:\n",
    "\n",
    "- `This is a very expensive vase.` (singular noun)\n",
    "\n",
    "- `The third floor in this mall sells vases.` (plural noun)\n",
    "\n",
    "Similarly, we need to find the root <u>**form of the noun**</u>, in this case `vase`. Although only differing in one letter, the ending `s`, algorithms treat them as distinct words. Hence, we need to find the root form of the noun so they can be treated as the same word as they refer to the same object in real life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "**Stemming** is the first way to find root forms of a word. It uses a fixed set of rules to shorten a word to its root form. `nltk` implements the **Porter Stemmer** and you can find the reference for the rules [here](http://www.nltk.org/howto/stem.html). Use `stemmer = PorterStemmer()` and then use the `stem()` method for each word to get its root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:50:29.397149Z",
     "start_time": "2018-09-12T14:50:29.391140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'consolid', 'credit', 'card', 'debt', 'incur', 'over', 'three', 'year', 'ago', 'and', 'have', 'a', 'concret', 'end', 'in', 'sight', 'is', 'more', 'motiv', '.', 'I', 'am', 'eagerli', 'strive', 'toward', 'becom', 'complet', 'debt', 'free', '.']\n"
     ]
    }
   ],
   "source": [
    "# Your turn: instantiate a stemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = []\n",
    "for t in ts1:\n",
    "    # Your turn: use stem(string) to find the root form of the word\n",
    "    u = stemmer.stem(t)\n",
    "    # Your turn: append the stemmed string to the list\n",
    "    stemmed_words.append(u)\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that some stemmed words are not valid english words. For example, `consolid` is not an English word. `motiv` and `eagerli` too. However, because of its relatively simple algorithm, some applications accept this form of the word and hence this algorithm is useful. Examples of implementations of stemming are in search engines as both the search term and text can be stemmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the original form of the sentence and the result after stemming for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.301991Z",
     "start_time": "2018-09-12T14:45:13.291949Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Raw          Stemming   \n",
      "   --------------    ------------\n",
      "              I                 I\n",
      "             am                am\n",
      "  consolidating          consolid\n",
      "         credit            credit\n",
      "           card              card\n",
      "           debt              debt\n",
      "       incurred             incur\n",
      "           over              over\n",
      "          three             three\n",
      "          years              year\n",
      "            ago               ago\n",
      "            and               and\n",
      "         having              have\n",
      "              a                 a\n",
      "       concrete           concret\n",
      "            end               end\n",
      "             in                in\n",
      "          sight             sight\n",
      "             is                is\n",
      "           more              more\n",
      "     motivating             motiv\n",
      "              .                 .\n",
      "              I                 I\n",
      "             am                am\n",
      "        eagerly           eagerli\n",
      "       striving            strive\n",
      "        towards            toward\n",
      "       becoming             becom\n",
      "     completely           complet\n",
      "           debt              debt\n",
      "           free              free\n"
     ]
    }
   ],
   "source": [
    "print(\"%15s   %15s   \" % (\"Raw\", \"Stemming\"))\n",
    "print(\"%15s-- %15s\" % (\"------------\", \"------------\"))\n",
    "for i in range(0, len(stemmed_words)-1):\n",
    "    print (\"%15s   %15s\" % (ts1[i], stemmed_words[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "\n",
    "Finally, before performing analysis, remove **stop words** from the sentence. A stop word is a word that usually appears in many texts, and hence do not hold any meaning. In signal processing language, this is referred to as <u>noise</u>. Refer to this [Github link](https://gist.github.com/sebleier/554280) for the list of stop words from `nltk`. `nltk.corpus.stopwords.words()` contains the list of stop words and if the word exists in them, ignore them.\n",
    "\n",
    "Recall that the logical expression\n",
    "\n",
    "```python\n",
    "    word in wordlist\n",
    "``` \n",
    "\n",
    "used in the context\n",
    "\n",
    "```python\n",
    "    if word in wordlist:\n",
    "``` \n",
    "is used to check if a word exists in a `list`. It returns `True` if the word is found and `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.607666Z",
     "start_time": "2018-09-12T14:45:13.304822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'consolid', 'credit', 'card', 'debt', 'incur', 'over', 'three', 'year', 'ago', 'and', 'have', 'a', 'concret', 'end', 'in', 'sight', 'is', 'more', 'motiv', '.', 'I', 'am', 'eagerli', 'strive', 'toward', 'becom', 'complet', 'debt', 'free', '.']\n",
      "\n",
      "['I', 'consolid', 'credit', 'card', 'debt', 'incur', 'three', 'year', 'ago', 'concret', 'sight', 'motiv', '.', 'I', 'eagerli', 'strive', 'toward', 'becom', 'complet', 'debt', 'free', '.']\n"
     ]
    }
   ],
   "source": [
    "final_list_of_words = []\n",
    "for l in stemmed_words:\n",
    "    # Your turn: Use not in stopwords.words() to check if the word is a stop word.\n",
    "    if l not in stopwords.words():\n",
    "        final_list_of_words.append(l)\n",
    "print(stemmed_words)\n",
    "print()\n",
    "print(final_list_of_words)\n",
    "# Your turn: How many words have been eliminated after removal of stop words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn: Text Normalisation** - Pick one of the following 2 sentences. This is also from the `loans-descs-1k.csv` dataset. \n",
    "- Convert the sentence to lowercase\n",
    "- Remove all special characters according to the pattern `[.®'&$’\\\"\\-()]`\n",
    "- Perform tokenisation, followed by stemming of your selected sentence\n",
    "- Remove stop words from the list of stemmed words\n",
    "\n",
    "Note: You can use `'''` to specify a multi-line string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.622744Z",
     "start_time": "2018-09-12T14:45:13.613561Z"
    }
   },
   "outputs": [],
   "source": [
    "s2 = '''I really need to consolidate my credit card debt so that I can become debt free. \n",
    "The interest is killing me and I'm just not getting anywhere with the balances. Help!'''\n",
    "\n",
    "s3 = '''Hello, I just closed on the house of my dreams and I would like to \n",
    "use this loan to pay off my high interest credit cards and build a deck on my home.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.635106Z",
     "start_time": "2018-09-12T14:45:13.627053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i really need to consolidate my credit card debt so that i can become debt free. \n",
      "the interest is killing me and i'm just not getting anywhere with the balances. help!\n"
     ]
    }
   ],
   "source": [
    "# Convert sentence to lower case\n",
    "s2_1 = s2.lower()\n",
    "print(s2_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.645358Z",
     "start_time": "2018-09-12T14:45:13.640415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i really need to consolidate my credit card debt so that i can become debt free  \n",
      "the interest is killing me and i m just not getting anywhere with the balances  help!\n"
     ]
    }
   ],
   "source": [
    "# regex substitution\n",
    "s2_2 = re.sub(\"[.®'&$’\\\"\\-()]\", \" \", s2_1)\n",
    "print(s2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.654516Z",
     "start_time": "2018-09-12T14:45:13.648443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'really', 'need', 'to', 'consolidate', 'my', 'credit', 'card', 'debt', 'so', 'that', 'i', 'can', 'become', 'debt', 'free', 'the', 'interest', 'is', 'killing', 'me', 'and', 'i', 'm', 'just', 'not', 'getting', 'anywhere', 'with', 'the', 'balances', 'help', '!']\n"
     ]
    }
   ],
   "source": [
    "# Tokenise\n",
    "s2_3 = nltk.word_tokenize(s2_2)\n",
    "print(s2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.672123Z",
     "start_time": "2018-09-12T14:45:13.662408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'realli', 'need', 'to', 'consolid', 'my', 'credit', 'card', 'debt', 'so', 'that', 'i', 'can', 'becom', 'debt', 'free', 'the', 'interest', 'is', 'kill', 'me', 'and', 'i', 'm', 'just', 'not', 'get', 'anywher', 'with', 'the', 'balanc', 'help', '!']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "st = PorterStemmer()\n",
    "s2_4 = [st.stem(s) for s in s2_3]\n",
    "print(s2_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T14:45:13.957619Z",
     "start_time": "2018-09-12T14:45:13.679091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['realli', 'need', 'consolid', 'credit', 'card', 'debt', 'becom', 'debt', 'free', 'interest', 'kill', 'get', 'anywher', 'balanc', 'help', '!']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "s2_5 = [s for s in s2_4 if s not in stopwords.words()]\n",
    "print(s2_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credits**\n",
    "- [Kaggle (Top Spotify Tracks of 2017)](https://www.kaggle.com/nadintamer/top-tracks-of-2017) for Dataset 1\n",
    "- [Kaggle (Lending Club Loan Data)](https://www.kaggle.com/wendykan/lending-club-loan-data) for Dataset 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
