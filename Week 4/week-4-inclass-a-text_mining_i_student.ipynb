{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DS102 | In Class Practice Week 4A - Text Mining I - Text Normalisation\n",
    "<hr>\n",
    "\n",
    "## Pre-Requisites\n",
    "\n",
    "1. Self-Study Week 3A - String Manipulation\n",
    "\n",
    "## Learning Objectives\n",
    "At the end of the lesson, you will be able to:\n",
    "\n",
    "- define **corpus**, **documents** and **terms** as part of the study of Natural Language Processing\n",
    "\n",
    "- define **tokenisation** as breaking a document into terms\n",
    "\n",
    "- understand the definition of **root form** of a word for verbs and nouns\n",
    "\n",
    "- identify **stemming** as a way to find the root form of a word\n",
    "\n",
    "- learn how to use **stop words** to filter out terms in a document that is not meaningful\n",
    "\n",
    "At the end of the lesson, you will be able to:\n",
    "\n",
    "- use `word_tokenize` from `nltk.tokenize` to break a document into a list of words\n",
    "\n",
    "- use `PorterStemmer`'s `stem` from `nltk.stem` to perform stemming of words\n",
    "\n",
    "- retrieve a list of stopwords defined in `stopwords.words()` from `nltk.corpus`\n",
    "\n",
    "### Datasets Required for this Self-Study\n",
    "1. `songs-100.csv`\n",
    "\n",
    "2. `loans-descs-1k.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\victo\\anaconda3\\lib\\site-packages (3.4)\n",
      "Requirement already satisfied: six in c:\\users\\victo\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in c:\\users\\victo\\anaconda3\\lib\\site-packages (from nltk) (3.4.0.3)\n",
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\victo\\anaconda3\\lib\\site-packages (from sklearn) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.8.2 in c:\\users\\victo\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.16.2)\n",
      "Requirement already satisfied: scipy>=0.13.3 in c:\\users\\victo\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.2.1)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\victo\\AppData\\Local\\pip\\Cache\\wheels\\76\\03\\bb\\589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:29.878653Z",
     "start_time": "2018-10-16T16:22:28.810728Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:29.886094Z",
     "start_time": "2018-10-16T16:22:29.880985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading corpora...\n",
      "Corpora download complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Use this cell to download all the required corpora first. Then, comment out this \n",
    "# block of code.\n",
    "print(\"Downloading corpora...\")    \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "print(\"Corpora download complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:29.894293Z",
     "start_time": "2018-10-16T16:22:29.890063Z"
    }
   },
   "outputs": [],
   "source": [
    "# If you are running this for the first time, use the previous cell to download all \n",
    "# the corpora before starting.\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus, Documents and Terms\n",
    "\n",
    "In lingustics (the study of language), a **corpus** is a collection of texts, represented by documents. A **document** contains multiple words and when strung together, produce meaning. Each word is called a **term**. \n",
    "\n",
    "Consider the following corpus of 4 documents from the 100 Song Titles dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:29.904641Z",
     "start_time": "2018-10-16T16:22:29.898461Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset 1, Credits at the end of the notebook\n",
    "song_titles = ['shape of you', \n",
    "               'paris', \n",
    "               'scared to be lonely', \n",
    "               'symphony feat zara larsson',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each <u>song title is a document</u>. The <u>collection of song titles forms the corpus</u>. The first document `shape of you` has <u>3 terms</u>. The second document `paris` has <u>1 term</u>. The third document `scared to be lonely` has <u>4 terms</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now read from `songs-100.csv`, a CSV file which, in this case is our corpus. There are $100$ documents in the song titles corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:29.931854Z",
     "start_time": "2018-10-16T16:22:29.909066Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset 1: Credits at the end of the notebook\n",
    "# Read from 'songs-100.csv' into song_titles_df\n",
    "song_titles_df = pd.read_csv('songs-100.csv')\n",
    "song_titles_df.head()\n",
    "# Use count() to find the number of documents in the corpus.\n",
    "song_titles_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation using `nltk`\n",
    "\n",
    "The Natural Language Toolkit or `nltk` library is a very powerful library used for natural language processing. We will be using `nltk.tokenize.word_tokenize()` to perform tokenisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:29.949700Z",
     "start_time": "2018-10-16T16:22:29.936413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>grade</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>7337222</td>\n",
       "      <td>8999285</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>D</td>\n",
       "      <td>Borrower added on 09/17/13 &gt; The wedding of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>7365470</td>\n",
       "      <td>9027579</td>\n",
       "      <td>31825.0</td>\n",
       "      <td>C</td>\n",
       "      <td>Borrower added on 09/15/13 &gt; Pay Off High Cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>676471</td>\n",
       "      <td>864466</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>G</td>\n",
       "      <td>Borrower added on 02/15/11 &gt; My husband and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>612322</td>\n",
       "      <td>785185</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Borrower added on 11/09/10 &gt; debt refi, alwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>7051350</td>\n",
       "      <td>8713086</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>B</td>\n",
       "      <td>Borrower added on 02/14/14 &gt; I am consolidat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  member_id  loan_amnt grade  \\\n",
       "296  7337222    8999285    10000.0     D   \n",
       "844  7365470    9027579    31825.0     C   \n",
       "110   676471     864466    20000.0     G   \n",
       "278   612322     785185    20000.0     F   \n",
       "951  7051350    8713086    13000.0     B   \n",
       "\n",
       "                                                  desc  \n",
       "296    Borrower added on 09/17/13 > The wedding of ...  \n",
       "844    Borrower added on 09/15/13 > Pay Off High Cr...  \n",
       "110    Borrower added on 02/15/11 > My husband and ...  \n",
       "278    Borrower added on 11/09/10 > debt refi, alwa...  \n",
       "951    Borrower added on 02/14/14 > I am consolidat...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset 2: Credits at the end of the notebook\n",
    "# Your turn: read the dataset from loans-descs-1k.csv into loan_descs_df \n",
    "loan_descs_df = pd.read_csv('loans-descs-1k.csv',  index_col=0)\n",
    "loan_descs_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:29.960490Z",
     "start_time": "2018-10-16T16:22:29.953145Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                                     7051350\n",
      "member_id                                              8713086\n",
      "loan_amnt                                                13000\n",
      "grade                                                        B\n",
      "desc           Borrower added on 02/14/14 > I am consolidat...\n",
      "Name: 951, dtype: object\n",
      "  Borrower added on 02/14/14 > I am consolidating credit card debt incurred over three years ago and having a concrete end in sight is more motivating.  I am eagerly striving towards becoming completely debt free.<br>\n"
     ]
    }
   ],
   "source": [
    "# Get the raw text in index = 4 and store it into s1. Hint: use df.loc[]\n",
    "s1 = loan_descs_df.iloc[4]\n",
    "\n",
    "# Uncomment this line to print s1\n",
    "print(s1)\n",
    "\n",
    "s_desc = s1['desc']\n",
    "print(s_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `re.sub()` to substitute the initial phrase `Borrower added on ...` with an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:29.977657Z",
     "start_time": "2018-10-16T16:22:29.964517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am consolidating credit card debt incurred over three years ago and having a concrete end in sight is more motivating.  I am eagerly striving towards becoming completely debt free.\n"
     ]
    }
   ],
   "source": [
    "r = 'Borrower added on \\d+/\\d+/\\d+ >|<br>'\n",
    "# Your turn: use re.sub() to substitute any string with the expression to an empty string.\n",
    "#\n",
    "s_desc1 = re.sub(r, '', s_desc)\n",
    "\n",
    "# Your turn: use strip() to remove leading and trailing spaces. Store the final variable as s2\n",
    "s_desc1 = s_desc1.strip()\n",
    "\n",
    "# Uncomment this line to print s2\n",
    "print(s_desc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tokenise a sentence, simply use `word_tokenize()` from the `nltk` library. This will convert the sentence into individual words AND special characters like full stops and commas.\n",
    "\n",
    "<div class=\"alert alert-info\"><b>DS102 Learning Guidelines: </b> There are other tokenisers from `nltk` like `RegexpTokenizer`. They will split text specifically when they find a particular regular expression. These Tokenizers are particularly useful for money expressions. However they are not in DS102 syllabus. Refer to [the API](https://www.nltk.org/api/nltk.tokenize.html) to find out more.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:30.002766Z",
     "start_time": "2018-10-16T16:22:29.980980Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'consolidating', 'credit', 'card', 'debt', 'incurred', 'over', 'three', 'years', 'ago', 'and', 'having', 'a', 'concrete', 'end', 'in', 'sight', 'is', 'more', 'motivating', '.', 'I', 'am', 'eagerly', 'striving', 'towards', 'becoming', 'completely', 'debt', 'free', '.']\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# Use word_tokenize(string) to convert the string into a list of tokens.\n",
    "# Assign this to a new variable called ts1\n",
    "#\n",
    "ts1 = word_tokenize(s_desc1)\n",
    "print(ts1)\n",
    "print(len(ts1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguistics - The root form of a word (verbs & nouns)\n",
    "Stemming is one way to find the **root form of a word**. We will only limit our discussion to verbs (action words) and nouns (naming words). First, consider the following 3 sentences that use different forms of the word `watch`:\n",
    "\n",
    "- `Larry watches television.` (singular present tense)\n",
    "\n",
    "- `The children watch television.` (simple tense / plural, present tense)\n",
    "\n",
    "- `My son is watching television.` (present participle tense / present continuous tense)\n",
    "\n",
    "- `My mum watched television with me.` (past tense)\n",
    "\n",
    "The word `watch` exists in 4 different <u>forms of the **verb**</u> as they exist in different tenses. However, algorithms treat them as **separate words** during analysis. Hence, we need to find the root form of the verb so they can be treated as the same word during analysis as they have the same meaning, in this case `watch`. \n",
    "\n",
    "`nltk` implements the **Porter Stemmer** and you can find the reference for the rules [here](http://www.nltk.org/howto/stem.html). Use `stemmer = PorterStemmer()` and then use the `stem()` method for each word to get its root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:30.013980Z",
     "start_time": "2018-10-16T16:22:30.005633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Larry', 'watches', 'television', '.']\n",
      "['larri', 'watch', 'televis', '.']\n"
     ]
    }
   ],
   "source": [
    "# Instantiate PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "ss_verbs = ['Larry watches television.', 'The children watch television.', \n",
    "       'My son is watching television.', 'My mum watched television with me.']\n",
    "\n",
    "\n",
    "ss1 = ss_verbs[0] # pull from corpus\n",
    "ss2 = word_tokenize(ss1) # break document to terms\n",
    "print(ss2)\n",
    "\n",
    "ss3 = []\n",
    "for s in ss2:\n",
    "    s_stemmed = stemmer.stem(s)\n",
    "    ss3.append(s_stemmed)\n",
    "print(ss3)\n",
    "\n",
    "\n",
    "# # (What meaningful comment can you write here?)\n",
    "# for s in ss_verbs:\n",
    "#     # (What meaningful comment can you write here?)\n",
    "#     for st in word_tokenize(s):\n",
    "#         # (What meaningful comment can you write here?)\n",
    "#         print(stemmer.stem(st))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the next 2 sentences:\n",
    "\n",
    "- `This is a very expensive vase.` (singular noun)\n",
    "\n",
    "- `The third floor in this mall sells vases.` (plural noun)\n",
    "\n",
    "Similarly, we need to find the root <u>form of the **noun**</u>, in this case `vase`. Although only differing in one letter, the ending `s`, algorithms treat them as distinct words. Hence, we need to find the root form of the noun so they can be treated as the same word as they refer to the same object in real life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:30.034818Z",
     "start_time": "2018-10-16T16:22:30.025259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'third', 'floor', 'in', 'thi', 'mall', 'sell', 'vase', '.']\n"
     ]
    }
   ],
   "source": [
    "ss_nouns = ['This is a very expensive vase.', \n",
    "            'The third floor in this mall sells vases.', ]\n",
    "\n",
    "ssn1 = ss_nouns[1]\n",
    "ssnlt = word_tokenize(ssn1)\n",
    "\n",
    "ssn2 = []\n",
    "\n",
    "# Exercise: Iterate through the list of sentences. Tokenise each sentense using word_tokenize().\n",
    "# Then for every term, print out its stemmed form using stemmer.stem(term)\n",
    "for s in ssnlt:\n",
    "    ssn2.append(stemmer.stem(s))\n",
    "    print(ssn2)\n",
    "    \n",
    "print(ssn2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Now, we apply the stemming step on the initial loans sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:30.055178Z",
     "start_time": "2018-10-16T16:22:30.046207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'consolidating', 'credit', 'card', 'debt', 'incurred', 'over', 'three', 'years', 'ago', 'and', 'having', 'a', 'concrete', 'end', 'in', 'sight', 'is', 'more', 'motivating', '.', 'I', 'am', 'eagerly', 'striving', 'towards', 'becoming', 'completely', 'debt', 'free', '.']\n"
     ]
    }
   ],
   "source": [
    "# Just to refresh our memory...\n",
    "print(ts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:30.071177Z",
     "start_time": "2018-10-16T16:22:30.059018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'consolid', 'credit', 'card', 'debt', 'incur', 'over', 'three', 'year', 'ago', 'and', 'have', 'a', 'concret', 'end', 'in', 'sight', 'is', 'more', 'motiv', '.', 'I', 'am', 'eagerli', 'strive', 'toward', 'becom', 'complet', 'debt', 'free', '.']\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_words = []\n",
    "for t in ts1:\n",
    "    # Use stemmer.stem() to find the root form of the word\n",
    "    t2 = stemmer.stem(t)\n",
    "    # Then amend this line to append the stemmed word into stemmed_words\n",
    "    stemmed_words.append(t2)\n",
    "    \n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, some stemmed words are not valid english words. For example, `consolid` is not an English word. `motiv` and `eagerli` too. However, because of its relatively simple algorithm, some applications accept this form of the word and hence this algorithm is useful. Examples of implementations of stemming are in search engines as both the search term and text can be stemmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the original form of the sentence and the result after stemming for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:30.094766Z",
     "start_time": "2018-10-16T16:22:30.075868Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Raw          Stemming   \n",
      "   --------------    ------------\n",
      "              I                 I\n",
      "             am                am\n",
      "  consolidating          consolid\n",
      "         credit            credit\n",
      "           card              card\n",
      "           debt              debt\n",
      "       incurred             incur\n",
      "           over              over\n",
      "          three             three\n",
      "          years              year\n",
      "            ago               ago\n",
      "            and               and\n",
      "         having              have\n",
      "              a                 a\n",
      "       concrete           concret\n",
      "            end               end\n",
      "             in                in\n",
      "          sight             sight\n",
      "             is                is\n",
      "           more              more\n",
      "     motivating             motiv\n",
      "              .                 .\n",
      "              I                 I\n",
      "             am                am\n",
      "        eagerly           eagerli\n",
      "       striving            strive\n",
      "        towards            toward\n",
      "       becoming             becom\n",
      "     completely           complet\n",
      "           debt              debt\n",
      "           free              free\n"
     ]
    }
   ],
   "source": [
    "# Uncomment this line to compare the results.\n",
    "\n",
    "print(\"%15s   %15s   \" % (\"Raw\", \"Stemming\"))\n",
    "print(\"%15s-- %15s\" % (\"------------\", \"------------\"))\n",
    "for i in range(0, len(stemmed_words)-1):\n",
    "    print (\"%15s   %15s\" % (ts1[i], stemmed_words[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "\n",
    "Finally, before performing analysis, remove **stop words** from the sentence. A stop word is a word that usually appears in many texts, and hence do not hold any meaning. In signal processing language, this is referred to as <u>noise</u>. Refer to this [Github link](https://gist.github.com/sebleier/554280) for the list of stop words from `nltk`. `nltk.corpus.stopwords.words()` contains the list of stop words and if the word exists in them, ignore them.\n",
    "\n",
    "Recall that \n",
    "\n",
    "```python\n",
    "    word in wordlist\n",
    "``` \n",
    "is used to check if a word is in a list. It returns `True` if the word is found and `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:30.335364Z",
     "start_time": "2018-10-16T16:22:30.099215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['consolid', 'credit', 'card', 'debt', 'incur', 'three', 'year', 'ago', 'concret', 'end', 'sight', 'motiv', '.', 'eagerli', 'strive', 'toward', 'becom', 'complet', 'debt', 'free', '.']\n"
     ]
    }
   ],
   "source": [
    "final_list_of_words = []\n",
    "for l in stemmed_words:\n",
    "    l_lower = l.lower()\n",
    "    if not l_lower in stopwords.words('english'):\n",
    "        final_list_of_words.append(l_lower)\n",
    "    # Your turn: Use not in stopwords.words('english') to check if the word \n",
    "    # is a stop word. If it isn't, then append to the final_list_of_words.\n",
    "print(final_list_of_words)\n",
    "    \n",
    "# Exersise: How many words have been eliminated after removal of stop words?\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn: Text Normalisation** - Pick one of the following 2 sentences. This is also from the `loans-descs-1k.csv` dataset. \n",
    "- Convert the sentence to lowercase\n",
    "- Remove all special characters according to the pattern `[.®'&$’\\\"\\-()]`\n",
    "- Perform tokenisation, followed by stemming of your selected sentence\n",
    "- Remove stop words from the list of stemmed words\n",
    "\n",
    "Note: You can use `'''` to specify a multi-line string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:30.343993Z",
     "start_time": "2018-10-16T16:22:30.339380Z"
    }
   },
   "outputs": [],
   "source": [
    "s2 = '''I really need to consolidate my credit card debt so that I can become debt free. \n",
    "The interest is killing me and I'm just not getting anywhere with the balances. Help!'''\n",
    "\n",
    "s3 = '''Hello, I just closed on the house of my dreams and I would like to \n",
    "use this loan to pay off my high interest credit cards and build a deck on my home.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:30.356083Z",
     "start_time": "2018-10-16T16:22:30.348819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, i just closed on the house of my dreams and i would like to \\nuse this loan to pay off my high interest credit cards and build a deck on my home.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Convert to lower case using lower()\n",
    "s2.lower()\n",
    "s3.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:30.366807Z",
     "start_time": "2018-10-16T16:22:30.358840Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Perform regex substitution to remove special characters.\n",
    "ss2 = re.sub('\\n', '', s2)\n",
    "ss3 = re.sub('\\n', '', s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:30.383700Z",
     "start_time": "2018-10-16T16:22:30.373540Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: use word_tokenize() to tokenise the sentence and get a list of terms.\n",
    "list2 = word_tokenize(ss2)\n",
    "list3 = word_tokenize(ss3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:30.407034Z",
     "start_time": "2018-10-16T16:22:30.396551Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'realli',\n",
       " 'need',\n",
       " 'to',\n",
       " 'consolid',\n",
       " 'my',\n",
       " 'credit',\n",
       " 'card',\n",
       " 'debt',\n",
       " 'so',\n",
       " 'that',\n",
       " 'I',\n",
       " 'can',\n",
       " 'becom',\n",
       " 'debt',\n",
       " 'free',\n",
       " '.',\n",
       " 'the',\n",
       " 'interest',\n",
       " 'is',\n",
       " 'kill',\n",
       " 'me',\n",
       " 'and',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'just',\n",
       " 'not',\n",
       " 'get',\n",
       " 'anywher',\n",
       " 'with',\n",
       " 'the',\n",
       " 'balanc',\n",
       " '.',\n",
       " 'help',\n",
       " '!']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Use stemmer.stem() to get the list of stemmed terms.\n",
    "list_2 = []\n",
    "list_3 = []\n",
    "\n",
    "for l in list2:\n",
    "    list_2.append(stemmer.stem(l))\n",
    "    \n",
    "for l in list3:\n",
    "    list_3.append(stemmer.stem(l))\n",
    "\n",
    "list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:30.437877Z",
     "start_time": "2018-10-16T16:22:30.413449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'realli', 'need', 'consolid', 'credit', 'card', 'debt', 'I', 'becom', 'debt', 'free', '.', 'interest', 'kill', 'I', \"'m\", 'get', 'anywher', 'balanc', '.', 'help', '!']\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Remove stop words. Remove the word if it appears in stopwords.words('english')\n",
    "final_2 = []\n",
    "final_3 = []\n",
    "\n",
    "for l in list_2:\n",
    "    if not l in stopwords.words('english'):\n",
    "        final_2.append(l)\n",
    "        \n",
    "print(final_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T16:22:30.450775Z",
     "start_time": "2018-10-16T16:22:30.441473Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finally, print() the sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credits**\n",
    "- [Kaggle](https://www.kaggle.com/nadintamer/top-tracks-of-2017) for Dataset 1\n",
    "- [Kaggle](https://www.kaggle.com/wendykan/lending-club-loan-data) for Dataset 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
